{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23d8b1c1-ca4c-48a5-97d6-9b4bcce2f45b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T15:22:52.541865Z",
     "start_time": "2024-10-27T15:22:52.371642Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from helpers import *\n",
    "from implementations import * \n",
    "from utilities.logistic_regression import * #todo specify with the ones we use\n",
    "from utilities.linear_regression import * #todo specify with the ones we use\n",
    "from utilities.cross_validation import * #todo specify with the ones we use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27630ed0-f71a-490f-84f6-728c96215804",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T15:24:02.377565Z",
     "start_time": "2024-10-27T15:22:52.556170Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "x_tr, x_te, y_tr, tr_id, te_id = load_csv_data('dataset', True)\n",
    "# Transform our y values from {-1,1} to {0,1} because thatâ€™s what logistic regression tests expect https://edstem.org/eu/courses/1605/discussion/134447\n",
    "y_tr[y_tr == -1] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6103f79-5c98-4f85-a8ba-9a60a660c1f5",
   "metadata": {},
   "source": [
    "### Data exploration and data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45306014-21c1-4841-9700-aa3f63e98ae9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T15:24:03.133927Z",
     "start_time": "2024-10-27T15:24:02.439072Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We analyze the percentage of NaN values in each feature to determine an appropriate cutoff threshold.\n",
    "Features with too many NaN values are less useful, but we donâ€™t want to remove too many features.\n",
    "\n",
    "number of features left for each threshold value:\n",
    "   5% nan: 115 -> 35% of features left\n",
    "  50% nan: 174 -> 54%\n",
    "  65% nan: 194 -> 60%\n",
    "  90% nan: 222 -> 69%\n",
    "we pick a threshold of 50% for now.\n",
    "TODO: test difference in accuracy with different thresholds {5, 50, 65, 90}%\n",
    "\"\"\"\n",
    "\n",
    "# Count the percentage of NaN values per feature\n",
    "nan_percentage_by_feature = (np.sum(np.isnan(x_tr), axis=0) / len(x_tr)) * 100\n",
    "\n",
    "# Plot the resulting distribution\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.hist(nan_percentage_by_feature, bins=100)\n",
    "plt.title(\"Distribution of NaN Percentage by feature\")\n",
    "plt.xlabel(\"NaN Percentage in the feature\")\n",
    "plt.ylabel(\"Number of features\")\n",
    "plt.xticks(np.arange(start=0, stop=101, step=5))\n",
    "\n",
    "# A threshold of maximum 50% of NaN values in a feature seems like a reasonable tradeoff\n",
    "MAX_NAN_PERCENTAGE = 50\n",
    "mask = nan_percentage_by_feature < MAX_NAN_PERCENTAGE\n",
    "# So we get rid of the features with more than MAX_NAN_PERCENTAGE % of NaN values\n",
    "x_tr_threshold = x_tr.copy()[:, mask]\n",
    "from_tr_threshold_to_original_tr_indices = np.where(mask)[0]\n",
    "# TODO: je ne crois pas quâ€™il y ait beson de copy(), on ne va pas changer x_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57b2f7e-d332-4d56-8a0d-a6e1f3406648",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T15:24:03.442316Z",
     "start_time": "2024-10-27T15:24:03.204753Z"
    }
   },
   "outputs": [],
   "source": [
    "# Our objective is to build a mask to separate numerical and co (categorical/ordinal) data. \n",
    "# First we explore the data to see what is the maximum number of unique values for a co feature.\n",
    "\n",
    "# Compute and plot the distribution of unique values \n",
    "unique_counts_and_indices = np.array([[len(np.unique(x_tr_threshold[:, i])),i] for i in range(x_tr_threshold.shape[1])])\n",
    "unique_counts = unique_counts_and_indices[:,0]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(unique_counts, bins=50) #todo naive visualization, just to have an idea\n",
    "plt.xlabel('Number of Unique Values')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Unique Values Across Features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "186f611f-c6fe-4a34-b721-7b4b39d773c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T15:24:03.458574Z",
     "start_time": "2024-10-27T15:24:03.454927Z"
    }
   },
   "outputs": [],
   "source": [
    "# we consider that a feature with less than 10 values can be considered co, and more than 60 values is \n",
    "# surely numerical (think about a human having to check 100 options)\n",
    "# Our goal is still to find the co feature with the most distinct values to use it as a threshold.\n",
    "unique_counts_and_indices_refined = unique_counts_and_indices[(unique_counts_and_indices[:,0] > 10) & (unique_counts_and_indices[:,0] < 60)]\n",
    "indices_to_consider = from_tr_threshold_to_original_tr_indices[unique_counts_and_indices_refined[:,1]] #we get back the original indexes of our x_tr to link our findings to the original feature indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd63db6-9b1f-4f02-bb9c-e46a1518a7a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T15:24:03.539664Z",
     "start_time": "2024-10-27T15:24:03.529609Z"
    }
   },
   "outputs": [],
   "source": [
    "x_label = np.array([\"_STATE\", \"FMONTH\", \"IDATE\", \"IMONTH\", \"IDAY\", \"IYEAR\", \"DISPCODE\", \"SEQNO\", \"_PSU\", \"CTELENUM\", \"PVTRESD1\", \"COLGHOUS\", \"STATERES\", \"CELLFON3\", \"LADULT\", \"NUMADULT\", \"NUMMEN\", \"NUMWOMEN\", \"CTELNUM1\", \"CELLFON2\", \"CADULT\", \"PVTRESD2\", \"CCLGHOUS\", \"CSTATE\", \"LANDLINE\", \"HHADULT\", \"GENHLTH\", \"PHYSHLTH\", \"MENTHLTH\", \"POORHLTH\", \"HLTHPLN1\", \"PERSDOC2\", \"MEDCOST\", \"CHECKUP1\", \"BPHIGH4\", \"BPMEDS\", \"BLOODCHO\", \"CHOLCHK\", \"TOLDHI2\", \"CVDSTRK3\", \"ASTHMA3\", \"ASTHNOW\", \"CHCSCNCR\", \"CHCOCNCR\", \"CHCCOPD1\", \"HAVARTH3\", \"ADDEPEV2\", \"CHCKIDNY\", \"DIABETE3\", \"DIABAGE2\", \"SEX\", \"MARITAL\", \"EDUCA\", \"RENTHOM1\", \"NUMHHOL2\", \"NUMPHON2\", \"CPDEMO1\", \"VETERAN3\", \"EMPLOY1\", \"CHILDREN\", \"INCOME2\", \"INTERNET\", \"WEIGHT2\", \"HEIGHT3\", \"PREGNANT\", \"QLACTLM2\", \"USEEQUIP\", \"BLIND\", \"DECIDE\", \"DIFFWALK\", \"DIFFDRES\", \"DIFFALON\", \"SMOKE100\", \"SMOKDAY2\", \"STOPSMK2\", \"LASTSMK2\", \"USENOW3\", \"ALCDAY5\", \"AVEDRNK2\", \"DRNK3GE5\", \"MAXDRNKS\", \"FRUITJU1\", \"FRUIT1\", \"FVBEANS\", \"FVGREEN\", \"FVORANG\", \"VEGETAB1\", \"EXERANY2\", \"EXRACT11\", \"EXEROFT1\", \"EXERHMM1\", \"EXRACT21\", \"EXEROFT2\", \"EXERHMM2\", \"STRENGTH\", \"LMTJOIN3\", \"ARTHDIS2\", \"ARTHSOCL\", \"JOINPAIN\", \"SEATBELT\", \"FLUSHOT6\", \"FLSHTMY2\", \"IMFVPLAC\", \"PNEUVAC3\", \"HIVTST6\", \"HIVTSTD3\", \"WHRTST10\", \"PDIABTST\", \"PREDIAB1\", \"INSULIN\", \"BLDSUGAR\", \"FEETCHK2\", \"DOCTDIAB\", \"CHKHEMO3\", \"FEETCHK\", \"EYEEXAM\", \"DIABEYE\", \"DIABEDU\", \"CAREGIV1\", \"CRGVREL1\", \"CRGVLNG1\", \"CRGVHRS1\", \"CRGVPRB1\", \"CRGVPERS\", \"CRGVHOUS\", \"CRGVMST2\", \"CRGVEXPT\", \"VIDFCLT2\", \"VIREDIF3\", \"VIPRFVS2\", \"VINOCRE2\", \"VIEYEXM2\", \"VIINSUR2\", \"VICTRCT4\", \"VIGLUMA2\", \"VIMACDG2\", \"CIMEMLOS\", \"CDHOUSE\", \"CDASSIST\", \"CDHELP\", \"CDSOCIAL\", \"CDDISCUS\", \"WTCHSALT\", \"LONGWTCH\", \"DRADVISE\", \"ASTHMAGE\", \"ASATTACK\", \"ASERVIST\", \"ASDRVIST\", \"ASRCHKUP\", \"ASACTLIM\", \"ASYMPTOM\", \"ASNOSLEP\", \"ASTHMED3\", \"ASINHALR\", \"HAREHAB1\", \"STREHAB1\", \"CVDASPRN\", \"ASPUNSAF\", \"RLIVPAIN\", \"RDUCHART\", \"RDUCSTRK\", \"ARTTODAY\", \"ARTHWGT\", \"ARTHEXER\", \"ARTHEDU\", \"TETANUS\", \"HPVADVC2\", \"HPVADSHT\", \"SHINGLE2\", \"HADMAM\", \"HOWLONG\", \"HADPAP2\", \"LASTPAP2\", \"HPVTEST\", \"HPLSTTST\", \"HADHYST2\", \"PROFEXAM\", \"LENGEXAM\", \"BLDSTOOL\", \"LSTBLDS3\", \"HADSIGM3\", \"HADSGCO1\", \"LASTSIG3\", \"PCPSAAD2\", \"PCPSADI1\", \"PCPSARE1\", \"PSATEST1\", \"PSATIME\", \"PCPSARS1\", \"PCPSADE1\", \"PCDMDECN\", \"SCNTMNY1\", \"SCNTMEL1\", \"SCNTPAID\", \"SCNTWRK1\", \"SCNTLPAD\", \"SCNTLWK1\", \"SXORIENT\", \"TRNSGNDR\", \"RCSGENDR\", \"RCSRLTN2\", \"CASTHDX2\", \"CASTHNO2\", \"EMTSUPRT\", \"LSATISFY\", \"ADPLEASR\", \"ADDOWN\", \"ADSLEEP\", \"ADENERGY\", \"ADEAT1\", \"ADFAIL\", \"ADTHINK\", \"ADMOVE\", \"MISTMNT\", \"ADANXEV\", \"QSTVER\", \"QSTLANG\", \"MSCODE\", \"_STSTR\", \"_STRWT\", \"_RAWRAKE\", \"_WT2RAKE\", \"_CHISPNC\", \"_CRACE1\", \"_CPRACE\", \"_CLLCPWT\", \"_DUALUSE\", \"_DUALCOR\", \"_LLCPWT\", \"_RFHLTH\", \"_HCVU651\", \"_RFHYPE5\", \"_CHOLCHK\", \"_RFCHOL\", \"_LTASTH1\", \"_CASTHM1\", \"_ASTHMS1\", \"_DRDXAR1\", \"_PRACE1\", \"_MRACE1\", \"_HISPANC\", \"_RACE\", \"_RACEG21\", \"_RACEGR3\", \"_RACE_G1\", \"_AGEG5YR\", \"_AGE65YR\", \"_AGE80\", \"_AGE_G\", \"HTIN4\", \"HTM4\", \"WTKG3\", \"_BMI5\", \"_BMI5CAT\", \"_RFBMI5\", \"_CHLDCNT\", \"_EDUCAG\", \"_INCOMG\", \"_SMOKER3\", \"_RFSMOK3\", \"DRNKANY5\", \"DROCDY3_\", \"_RFBING5\", \"_DRNKWEK\", \"_RFDRHV5\", \"FTJUDA1_\", \"FRUTDA1_\", \"BEANDAY_\", \"GRENDAY_\", \"ORNGDAY_\", \"VEGEDA1_\", \"_MISFRTN\", \"_MISVEGN\", \"_FRTRESP\", \"_VEGRESP\", \"_FRUTSUM\", \"_VEGESUM\", \"_FRTLT1\", \"_VEGLT1\", \"_FRT16\", \"_VEG23\", \"_FRUITEX\", \"_VEGETEX\", \"_TOTINDA\", \"METVL11_\", \"METVL21_\", \"MAXVO2_\", \"FC60_\", \"ACTIN11_\", \"ACTIN21_\", \"PADUR1_\", \"PADUR2_\", \"PAFREQ1_\", \"PAFREQ2_\", \"_MINAC11\", \"_MINAC21\", \"STRFREQ_\", \"PAMISS1_\", \"PAMIN11_\", \"PAMIN21_\", \"PA1MIN_\", \"PAVIG11_\", \"PAVIG21_\", \"PA1VIGM_\", \"_PACAT1\", \"_PAINDX1\", \"_PA150R2\", \"_PA300R2\", \"_PA30021\", \"_PASTRNG\", \"_PAREC1\", \"_PASTAE1\", \"_LMTACT1\", \"_LMTWRK1\", \"_LMTSCL1\", \"_RFSEAT2\", \"_RFSEAT3\", \"_FLSHOT6\", \"_PNEUMO2\", \"_AIDTST3\"])\n",
    "x_label[indices_to_consider]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ff4420-5704-4dea-bf09-49db90d94c3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T15:24:03.613575Z",
     "start_time": "2024-10-27T15:24:03.606948Z"
    }
   },
   "outputs": [],
   "source": [
    "''' co = categorical/ordinal, n = numerical\n",
    "_STATE: 53 co\n",
    "FMONTH: 12 \n",
    "IMONTH: 12 \n",
    "IDAY: 31 \n",
    "PHYSHLTH: 32 \n",
    "MENTHLTH: 32 \n",
    "POORHLTH: 32 \n",
    "CHILDREN: 11\n",
    "INCOME2: 11\n",
    "HEIGHT3: 57 n\n",
    "ALCDAY5: 38\n",
    "FRUITJU1: 50 \n",
    "FVBEANS: 49\n",
    "FVGREEN: 56 wtf is this format ðŸ˜‚\n",
    "FVORANG: 52\n",
    "STRENGTH: 49\n",
    "_AGEG5YR: 14\n",
    "HTIN4: 32\n",
    "HTM4: 46\n",
    "DROCDY3_: 34\n",
    "FTJUDA1_: 43\n",
    "FRUTDA1_: 59 n\n",
    "BEANDAY_: 42\n",
    "GRENDAY_: 48\n",
    "ORNGDAY_: 43\n",
    "VEGEDA1_: 58 I honestly dont know\n",
    "METVL11_: 29\n",
    "METVL21_: 28\n",
    "PAFREQ1_: 55\n",
    "STRFREQ_: 44\n",
    "\n",
    "\n",
    "--> The answer was right under our nose ! The co feature with the most distinct values is _STATE.  \n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52b96a3e-5bed-4a5a-888b-2cb3b051af1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T15:24:03.672398Z",
     "start_time": "2024-10-27T15:24:03.668430Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "After methodical data exploration and analysis, we observed that the co feature that had \n",
    "the most distinct values (53) was _STATE which is our first feature. \n",
    "\n",
    "We consider every feature that has 53 distinct values or less as categorical or ordinal, and every feature that \n",
    "has 54 values or more as numerical.  todo this is with THRESHOLD = 50 ! update if we change threshold. or dont. you are a free man\n",
    "'''\n",
    "\n",
    "THRESHOLD_CO = 53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "605596de-e9b6-40c8-8861-ac6a86d1bba7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T15:24:03.722800Z",
     "start_time": "2024-10-27T15:24:03.718796Z"
    }
   },
   "outputs": [],
   "source": [
    "index_of_co_features = np.where(unique_counts_and_indices[:,0] <= THRESHOLD_CO)[0]\n",
    "index_of_numerical_features = np.where(unique_counts_and_indices[:,0] > THRESHOLD_CO)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd46b60b-12d3-4b4a-8d84-c4e3a5c64663",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T15:24:03.795752Z",
     "start_time": "2024-10-27T15:24:03.791115Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Now that we have isolated co features from numerical features, we can\n",
    "- Impute missing values represented by a nan correctly (ex: replace nan with mode if co, replace nan with mean if numerical)\n",
    "- Encode correctly our co features by:\n",
    "    - One hot encoding like brutes\n",
    "    - Keeping the original feature to make sure we dont use the ordinality of the ordinal features, as we dont make a difference between ordinal and categorical features.\n",
    "\n",
    "todo: manual cleaning to get rid of residual nan (for example: 9999 values that still mean nan). Maybe we get rid of them when we get rid of outliers later ?\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "987b7e1e-5e72-4d88-bc1e-36991b3a8f11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T15:24:03.906413Z",
     "start_time": "2024-10-27T15:24:03.868469Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Starting here, we assume that every unusable value is a nan.\n",
    "\n",
    "How do we impute missing values ?\n",
    "We can take two routes:\n",
    "1) Univariate imputation of missing values (we only consider the values of the related feature to impute missing values)\n",
    "    - For numerical features: We replace by the mean or the median \n",
    "    - For co features: We replace nan values by a unique value to distinguish them\n",
    "2) Multivariate imputation of missing values (we take into account all of the features to impute missing values)\n",
    "    - Multiple techniques exist (KNN based, regression based...)\n",
    "\n",
    "\n",
    "You have to agree that option 2 has a lot more style (and relevance). But it could also be a pain to implement with numpy \n",
    "(one usually uses scikit learn for this purpose) and as we did not see anything in class, \n",
    "all that we do is extra so we will go the easy route (1) for now. \n",
    "\n",
    "todo can we try 2) Multivariate imputation\n",
    "'''\n",
    "\n",
    "# missing values imputation for numerical features\n",
    "# replace nan values with median (more robust to outliers) todo check with mean\n",
    "median_x_tr_num = np.nanmedian(x_tr_threshold[:, index_of_numerical_features], axis=0) #shape is (37,) as expected\n",
    "x_tr_threshold[:, index_of_numerical_features] = np.nan_to_num(x_tr_threshold[:, index_of_numerical_features], nan=median_x_tr_num)\n",
    "\n",
    "# missing values imputation for co features\n",
    "# replace nan values with feature max + 1  (todo arbitrary to make it unique, better than replacing with mode in my opinion. How to improve it ?) \n",
    "new_max_for_nan = np.nanmax(x_tr_threshold[:, index_of_co_features], axis=0) + 1\n",
    "x_tr_threshold[:, index_of_co_features] = np.nan_to_num(x_tr_threshold[:, index_of_co_features], nan=new_max_for_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae4d5ae-1d0a-43cd-8a67-d1ef0c2ed38c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T15:24:03.928446Z",
     "start_time": "2024-10-27T15:24:03.922463Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "we test if our method work with a toy example\n",
    "'''\n",
    "#numerical\n",
    "test_missing_values_imputation_numerical = np.array([\n",
    "    [1, np.nan, 3, np.nan],\n",
    "    [4, 5, np.nan, 7],\n",
    "    [np.nan, 2, 6, 8],\n",
    "    [9, np.nan, np.nan, 10]\n",
    "])\n",
    "print('Original array:\\n' +str(test_missing_values_imputation_numerical))\n",
    "print('\\nnumber of nan: ' +str(np.isnan(test_missing_values_imputation_numerical).sum()))\n",
    "median = np.nanmedian(test_missing_values_imputation_numerical, axis=0)\n",
    "print('\\nmedian vector: ' + str(median))\n",
    "test_missing_values_imputation_numerical = np.nan_to_num(test_missing_values_imputation_numerical, nan=median)\n",
    "print('\\nNew array:\\n' + str(test_missing_values_imputation_numerical))\n",
    "#we see that all the \n",
    "print('\\nNumber of NaN left: ' + str(np.isnan(test_missing_values_imputation_numerical).sum()))\n",
    "\n",
    "#categorical should be the same\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56686cd2-7f35-462d-b7a0-dea5e6cc3cb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T15:24:05.983968Z",
     "start_time": "2024-10-27T15:24:03.975807Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Feature transformation\n",
    "\n",
    "- For numerical features: We standardize by substracting the mean and dividing by the std (todo we can also normalize them, which is better ? (ex: search min max normalization))\n",
    "- For co features: \n",
    "    1) We add the one hot encoding while also keeping the original feature (to keep relations of ordinality for ordinal features)\n",
    "    2) Can we handle nan values this way ? We need to check the results. (todo give me your opinion) \n",
    "'''\n",
    "\n",
    "#numerical features standardization\n",
    "x_tr_num_mean = np.mean(x_tr_threshold[:, index_of_numerical_features], axis = 0)\n",
    "x_tr_num_std = np.std(x_tr_threshold[:, index_of_numerical_features], axis = 0)\n",
    "x_tr_threshold[:, index_of_numerical_features] = (x_tr_threshold[:, index_of_numerical_features] - x_tr_num_mean) / x_tr_num_std\n",
    "\n",
    "#co features encoding \n",
    "N = len(x_tr)\n",
    "for idx in index_of_co_features:\n",
    "    unique_values = np.unique(x_tr_threshold[:, idx])\n",
    "    n_col = len(unique_values)\n",
    "    one_hot_feature = np.zeros((N, n_col)) #todo say dtype is int ?\n",
    "\n",
    "    #Here the objective is to go from the value to the index in the array of unique values\n",
    "    val_to_index = {value: index for index, value in enumerate(unique_values)}\n",
    "\n",
    "    for i in range(N):\n",
    "        value = x_tr_threshold[i, idx]\n",
    "        val_index = val_to_index[value]\n",
    "        one_hot_feature[i, val_index] = 1\n",
    "\n",
    "    #Finally, we concatenate these new features to our x_tr_threshold matrix\n",
    "    x_tr_threshold = np.hstack((x_tr_threshold, one_hot_feature))\n",
    "\n",
    "#finally we also standardize the original co features which can have funky values \n",
    "x_tr_co_mean = np.mean(x_tr_threshold[:, index_of_co_features], axis = 0)\n",
    "x_tr_co_std = np.std(x_tr_threshold[:, index_of_co_features], axis = 0)\n",
    "\n",
    "x_tr_co_std = np.where(x_tr_co_std == 0, 1, x_tr_co_std) #we replace 0 values by 1 to avoid invalid division,\n",
    "x_tr_threshold[:, index_of_co_features] = (x_tr_threshold[:, index_of_co_features] - x_tr_co_mean) / x_tr_co_std\n",
    "    \n",
    "x_tr_threshold.shape # We now have 1542 features. You need to copy x_tr again to retry, else new columns will be added on the already updated x_tr_threshold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57556382-9b36-4a3d-ac76-c999905d0521",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T15:24:06.017349Z",
     "start_time": "2024-10-27T15:24:06.002093Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "We are now expecting no nan in our dataset whatsoever.\n",
    "'''\n",
    "np.isnan(x_tr_threshold).any() #good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e5d10e-4872-4997-8522-57ac4b681a3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T15:24:06.073504Z",
     "start_time": "2024-10-27T15:24:06.068005Z"
    }
   },
   "outputs": [],
   "source": [
    "''' \n",
    "todo: Check for outliers \n",
    "\n",
    "that are not possible according to the documentation through boxplots or condition enforcement \n",
    "(schematic example: enforce that weights are inferior to 1000 and if not cap them to the max(1000)) \n",
    "we can also z-score normalization and check outside [-3,3] which indicates outlier (understand method). We need to \n",
    "make it as automatic as possible, this seems like a better idea.\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9094981-2c69-4bb5-8eb5-a7bac7a3e9a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T15:24:06.150706Z",
     "start_time": "2024-10-27T15:24:06.145944Z"
    }
   },
   "outputs": [],
   "source": [
    "''' todo to consider\n",
    "Non-linear Feature expension\n",
    "\n",
    "We have already so many features that it may always lead to overfitting. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ab82b5b-3051-46be-83f9-7e35e70a5831",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T15:24:06.227974Z",
     "start_time": "2024-10-27T15:24:06.208727Z"
    }
   },
   "outputs": [],
   "source": [
    "x_tr_clean = x_tr_threshold.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58473ea6-2c79-4734-b820-2ea2f4c4ec18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T15:24:06.323845Z",
     "start_time": "2024-10-27T15:24:06.292861Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Separate the data\n",
    "\n",
    "So that we don't have to systematically use the AI crowd feedback, we use 10% of our training dataset as test. We will \n",
    "later need to add a portion of our data to the validation set.\n",
    "'''\n",
    "\n",
    "tr_size = 0.9  # 90% of the data for the training set\n",
    "random_seed = 0 #so we can compare results. But we should get rid of this at some point to ensure robustness of method\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "N = x_tr_clean.shape[0]\n",
    "indices = np.random.permutation(N)\n",
    "\n",
    "tr_set_size = int(N * tr_size)\n",
    "tr_indices = indices[:tr_set_size]\n",
    "te_indices = indices[tr_set_size:]\n",
    "\n",
    "#todo improve choice of variables\n",
    "X_train, X_test = x_tr_clean[tr_indices], x_tr_clean[te_indices]\n",
    "y_train, y_test = y_tr[tr_indices], y_tr[te_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6299af8c-fc3d-46fe-9056-765e307a3e8b",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62606314-4843-4380-866b-e22a715e9888",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T15:24:09.159661Z",
     "start_time": "2024-10-27T15:24:06.336434Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Feature selection: \n",
    "\n",
    "Using a correlation matrix, we would capture pairwise correlation relationships\n",
    "but our dataset captures correlations involving the interaction between three or more variables (ex: dates) \n",
    "      \n",
    "\n",
    "We can use another approach: We do a regularized logistic regression, which forces the model to give more weight to\n",
    "highly predictive features, and close to 0 weight to useless features (as the model tries to spare the amount of weight).\n",
    "\n",
    "We train our model using all of the features a first time, then compare the absolute values of the weights and get rid of\n",
    "the features that have an associated weight that has a low enough absolute value (they were not deemed predictive during the regularized regression)\n",
    "\n",
    "As we have seen in ADA, the lower the absolute value of a weight, the less impact a change in the related input feature will have on the prediction.\n",
    "'''\n",
    "\n",
    "def build_tx(x):\n",
    "    return np.c_[np.ones((x.shape[0], 1)), x]\n",
    "\n",
    "max_iter = 100 #todo all arbitrary based on notebook. validate the hyperparameters. \n",
    "gamma = 0.5\n",
    "lambda_ = 0.0005\n",
    "tx = build_tx(X_train)\n",
    "initial_w = np.zeros((tx.shape[1]))\n",
    "\n",
    "w, loss = reg_logistic_regression(y_train, tx, lambda_, initial_w, max_iter, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb327fd-92ca-4e40-a637-473d7ea8ce8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T15:24:10.447392Z",
     "start_time": "2024-10-27T15:24:09.184250Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Then we retrain our model with only the predictive features\n",
    "'''\n",
    "\n",
    "w_abs = np.abs(w)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(range(len(w_abs)), w_abs)\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Value of w abs\")\n",
    "plt.title(\"Values in vector w\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60118e17-ddf4-4a9d-9b40-adbfcaa9a6bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T16:34:07.436025Z",
     "start_time": "2024-10-27T16:34:07.429216Z"
    }
   },
   "outputs": [],
   "source": [
    "def fit_with_t(w_abs, t, y_test, y_train, tx_test, tx, lambda_, initial_w, max_iter, gamma):\n",
    "    w, train_loss = reg_logistic_regression(y_train, tx, lambda_, initial_w, max_iter, gamma)\n",
    "\n",
    "    # Sigmoid gives a value between 0 and 1\n",
    "    test_probs = sigmoid(tx_test @ w)\n",
    "    # We round to the nearest to get our prediction in {0,1}\n",
    "    # (which we will transform later into {-1,1} for submission\n",
    "    test_preds = np.round(test_probs)\n",
    "    test_error_rate = np.count_nonzero(y_test - test_preds) / len(y_test)\n",
    "    test_accuracy = 1 - test_error_rate\n",
    "    test_f1 = compute_f1_score(y_test, test_preds)\n",
    "\n",
    "    return test_f1, train_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d90ea717-81cd-4eb1-91c9-f79f2bca2872",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T16:34:41.026882Z",
     "start_time": "2024-10-27T16:34:11.986454Z"
    }
   },
   "outputs": [],
   "source": [
    "# We want to simplify the model, get better performance and reduce overfitting risk,\n",
    "# by removing the features that do not contribute much to the predictions.\n",
    "\n",
    "# So we only select the top P percent of features that contribute the most to the\n",
    "# predictions, judging by the weight they have in the weight vector w we learned with\n",
    "# logistic regression.\n",
    "\n",
    "# The percentile P is the minimum weight value we require to keep P percent of the features.\n",
    "# We select this P methodically by testing a wide range of possible such thresholds.\n",
    "\n",
    "w_abs = np.abs(w)\n",
    "tx_test = build_tx(X_test)\n",
    "# We want to test keeping between 100% and 5% of the most significant features\n",
    "# This means having a threshold weight between the 0th and the 95th percentile of weight values \n",
    "percentiles_to_test = np.arange(0, 100, 5)\n",
    "threshold_w = [np.percentile(w_abs, p) for p in percentiles_to_test]\n",
    "\n",
    "test_f1_scores = []\n",
    "train_losses = []\n",
    "test_accuracies = []\n",
    "for t in threshold_w: #todo add bias to tx_t and tx_test_t and w again \n",
    "    features_to_keep = w_abs > t\n",
    "    #we define a unique variable tx_t for each iteration not to mess with the original tx\n",
    "    tx_t = tx[:, features_to_keep]\n",
    "    initial_w = np.zeros(tx_t.shape[1])\n",
    "    tx_test_t = tx_test[:, features_to_keep] \n",
    "\n",
    "    test_f1, train_loss, test_accuracy = fit_with_t(w_abs, t, y_test, y_train, tx_test_t, tx_t, lambda_, initial_w, max_iter, gamma)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    test_f1_scores.append(test_f1)\n",
    "    test_accuracies.append(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebd8aec-b8a8-4e7b-ab25-d05dbc50fc55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T16:36:24.151156Z",
     "start_time": "2024-10-27T16:36:23.772578Z"
    }
   },
   "outputs": [],
   "source": [
    "#plot threshold and score relation\n",
    "\n",
    "# We kept the top x% of features contributing most to predictions\n",
    "percent_of_features_kept = 100 - percentiles_to_test\n",
    "\n",
    "plt.plot(percent_of_features_kept, test_f1_scores, marker='o')\n",
    "plt.xlabel(\"Percentage of features kept\")\n",
    "plt.ylabel(\"Test F1 Score\")\n",
    "plt.title(\"Test F1 Score as we keep more or less features\")\n",
    "plt.xticks(percent_of_features_kept)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(percent_of_features_kept, train_losses, marker='o')\n",
    "plt.xlabel(\"Percentage of features kept\")\n",
    "plt.ylabel(\"Train Loss\")\n",
    "plt.title(\"Train Loss as we keep more or less features\")\n",
    "plt.xticks(percent_of_features_kept)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(percent_of_features_kept, test_accuracies, marker='o')\n",
    "plt.xlabel(\"Percentage of features kept\")\n",
    "plt.ylabel(\"Test Accuracy\")\n",
    "plt.title(\"Test Accuracy as we keep more or less features\")\n",
    "plt.xticks(percent_of_features_kept)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1d5615",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037c2872",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = 5 * 10 ** (-np.arange(1, 4, dtype=np.float64))\n",
    "initial_w = np.zeros((tx.shape[1]))\n",
    "max_its = [100, 200, 350, 500]\n",
    "gammas = [0.1, 0.5, 1]\n",
    "fold_nums = 4\n",
    "# WARNING: Takes a LOT of time (6 min)\n",
    "best_loss, best_hps, losses_per_hp = cross_validation_reg_logistic_regression(y_train, tx, lambdas, initial_w, max_its, gammas, fold_nums, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7518124",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best loss: {:.2f}\\nBest hyperparameters:\\n\\t{}\".format(best_loss, \"\\n\\t\".join([\"{:10s} {}\".format(k, v) for k, v in best_hps.items()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d84eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Best loss: 0.23\n",
    "Best hyperparameters:\n",
    "\tlambda_    0.005\n",
    "\tmax_it     100\n",
    "\tgamma      0.1\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3bfb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We now study the average loss for each hyperparameter value\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f9a011",
   "metadata": {},
   "source": [
    "### Average losses for `lambda_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47b8a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set_title(\"Average loss for each value of lambda\")\n",
    "ax.set_xlabel(\"Lambda (log scale)\")\n",
    "ax.set_ylabel(\"Average loss\")\n",
    "\n",
    "losses = losses_per_hp['lambda_']\n",
    "\n",
    "ax.plot([np.log(k) for k in losses.keys()], [sum(ls) / len(ls) for ls in losses.values()], 'o-')\n",
    "ax.set_xbound(-6,0)\n",
    "ax.set_xticks([np.log(k) for k in losses.keys()], losses.keys())\n",
    "fig.savefig('average-loss-lambda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b4be24",
   "metadata": {},
   "source": [
    "### Average losses for `max_iters`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10d8073",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_title(\"Average loss for each value of max iterations\")\n",
    "ax.set_xlabel(\"Max iterations\")\n",
    "ax.set_ylabel(\"Average loss\")\n",
    "\n",
    "losses = losses_per_hp['max_it']\n",
    "\n",
    "ax.plot(losses.keys(), [sum(ls) / len(ls) for ls in losses.values()], 'o-')\n",
    "ax.set_xbound(0, 600)\n",
    "ax.set_xticks(list(losses.keys()))\n",
    "fig.savefig('average-loss-max-iters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4940e3d5",
   "metadata": {},
   "source": [
    "### Average losses for `gamma`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f739d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_title(\"Average loss for each value of gamma\")\n",
    "ax.set_xlabel(\"Gamma (learning rate)\")\n",
    "ax.set_ylabel(\"Average loss\")\n",
    "\n",
    "losses = losses_per_hp['gamma']\n",
    "\n",
    "ax.plot(losses.keys(), [sum(ls) / len(ls) for ls in losses.values()], 'o-')\n",
    "ax.set_xbound(0, 1)\n",
    "ax.set_xticks(list(losses.keys()))\n",
    "fig.savefig('average-loss-gamma')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
