{
 "cells": [
  {
   "cell_type": "code",
   "id": "23d8b1c1-ca4c-48a5-97d6-9b4bcce2f45b",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from helpers import *\n",
    "from implementations import * \n",
    "from utilities.logistic_regression import * #todo specify with the ones we use\n",
    "from utilities.linear_regression import * #todo specify with the ones we use\n",
    "from utilities.cross_validation import * #todo specify with the ones we use"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "27630ed0-f71a-490f-84f6-728c96215804",
   "metadata": {},
   "source": [
    "# Load the dataset\n",
    "x_tr_original, x_te_original, y_tr, tr_id, te_id = load_csv_data('dataset', False)\n",
    "# Transform our y values from {-1,1} to {0,1} because thatâ€™s what logistic regression tests expect https://edstem.org/eu/courses/1605/discussion/134447\n",
    "y_tr[y_tr == -1] = 0"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "86f1340504ad4f03",
   "metadata": {},
   "source": [
    "# Make a copy so we can just rerun this cell to start over instead of re-loading the dataset from disk, which is long\n",
    "x_tr = x_tr_original#.copy()\n",
    "x_te = x_te_original#.copy()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c6103f79-5c98-4f85-a8ba-9a60a660c1f5",
   "metadata": {},
   "source": [
    "### Data exploration and data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "id": "45306014-21c1-4841-9700-aa3f63e98ae9",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "We analyze the percentage of NaN values in each feature to determine an appropriate cutoff threshold.\n",
    "Features with too many NaN values are less useful, but we donâ€™t want to remove too many features.\n",
    "\n",
    "number of features left for each threshold value:\n",
    "   5% nan: 115 -> 35% of features left\n",
    "  50% nan: 174 -> 54%\n",
    "  65% nan: 194 -> 60%\n",
    "  90% nan: 222 -> 69%\n",
    "we pick a threshold of 50% for now.\n",
    "TODO: test difference in accuracy with different thresholds {5, 50, 65, 90}%\n",
    "\"\"\"\n",
    "\n",
    "def filter_features_with_too_many_nans(x_tr, x_te):\n",
    "    # Count the percentage of NaN values per feature\n",
    "    nan_percentage_by_feature = (np.sum(np.isnan(x_tr), axis=0) / len(x_tr)) * 100\n",
    "    \n",
    "    # Plot the resulting distribution\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.hist(nan_percentage_by_feature, bins=100)\n",
    "    plt.title(\"Distribution of NaN Percentage by feature\")\n",
    "    plt.xlabel(\"NaN Percentage in the feature\")\n",
    "    plt.ylabel(\"Number of features\")\n",
    "    plt.xticks(np.arange(start=0, stop=101, step=5))\n",
    "    \n",
    "    # A threshold of maximum 50% of NaN values in a feature seems like a reasonable tradeoff\n",
    "    MAX_NAN_PERCENTAGE = 50\n",
    "    mask = nan_percentage_by_feature < MAX_NAN_PERCENTAGE\n",
    "    # So we get rid of the features with more than MAX_NAN_PERCENTAGE % of NaN values\n",
    "    x_tr = x_tr.copy()[:, mask]\n",
    "    x_te = x_te.copy()[:, mask]\n",
    "    from_nan_filtered_to_original_features = np.where(mask)[0]\n",
    "    return x_tr, x_te, from_nan_filtered_to_original_features\n",
    "\n",
    "x_tr, x_te, from_nan_filtered_to_original_features = filter_features_with_too_many_nans(x_tr, x_te)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d57b2f7e-d332-4d56-8a0d-a6e1f3406648",
   "metadata": {},
   "source": [
    "# Our objective is to build a mask to separate numerical and co (categorical/ordinal) data. \n",
    "# First we explore the data to see what is the maximum number of unique values for a co feature.\n",
    "\n",
    "# Compute and plot the distribution of unique values \n",
    "unique_counts_and_indices = np.array([[len(np.unique(x_tr[:, i])),i] for i in range(x_tr.shape[1])])\n",
    "unique_counts = unique_counts_and_indices[:,0]\n",
    "\n",
    "plt.hist(unique_counts, bins=50) # Naive visualization, just to have an idea\n",
    "plt.xlabel('Number of Unique Values')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Unique Values Across Features')\n",
    "plt.show()\n",
    "\n",
    "# We consider that a feature with less than 10 values can be considered categorical/ordinal (co),\n",
    "# and one with more than distinct 60 values is surely numerical (think about a human having to\n",
    "# pick 100 options in a survey)\n",
    "\n",
    "# Our goal is to find the co feature with the most distinct values to use it as a threshold\n",
    "unique_counts_and_indices_refined = unique_counts_and_indices[(unique_counts_and_indices[:,0] > 10) & (unique_counts_and_indices[:,0] < 60)]\n",
    "# We get back the original indexes of our x_tr to link our findings to the original feature indices\n",
    "indices_to_consider = from_nan_filtered_to_original_features[unique_counts_and_indices_refined[:,1]]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2bd63db6-9b1f-4f02-bb9c-e46a1518a7a7",
   "metadata": {},
   "source": [
    "x_label = np.array([\"_STATE\", \"FMONTH\", \"IDATE\", \"IMONTH\", \"IDAY\", \"IYEAR\", \"DISPCODE\", \"SEQNO\", \"_PSU\", \"CTELENUM\", \"PVTRESD1\", \"COLGHOUS\", \"STATERES\", \"CELLFON3\", \"LADULT\", \"NUMADULT\", \"NUMMEN\", \"NUMWOMEN\", \"CTELNUM1\", \"CELLFON2\", \"CADULT\", \"PVTRESD2\", \"CCLGHOUS\", \"CSTATE\", \"LANDLINE\", \"HHADULT\", \"GENHLTH\", \"PHYSHLTH\", \"MENTHLTH\", \"POORHLTH\", \"HLTHPLN1\", \"PERSDOC2\", \"MEDCOST\", \"CHECKUP1\", \"BPHIGH4\", \"BPMEDS\", \"BLOODCHO\", \"CHOLCHK\", \"TOLDHI2\", \"CVDSTRK3\", \"ASTHMA3\", \"ASTHNOW\", \"CHCSCNCR\", \"CHCOCNCR\", \"CHCCOPD1\", \"HAVARTH3\", \"ADDEPEV2\", \"CHCKIDNY\", \"DIABETE3\", \"DIABAGE2\", \"SEX\", \"MARITAL\", \"EDUCA\", \"RENTHOM1\", \"NUMHHOL2\", \"NUMPHON2\", \"CPDEMO1\", \"VETERAN3\", \"EMPLOY1\", \"CHILDREN\", \"INCOME2\", \"INTERNET\", \"WEIGHT2\", \"HEIGHT3\", \"PREGNANT\", \"QLACTLM2\", \"USEEQUIP\", \"BLIND\", \"DECIDE\", \"DIFFWALK\", \"DIFFDRES\", \"DIFFALON\", \"SMOKE100\", \"SMOKDAY2\", \"STOPSMK2\", \"LASTSMK2\", \"USENOW3\", \"ALCDAY5\", \"AVEDRNK2\", \"DRNK3GE5\", \"MAXDRNKS\", \"FRUITJU1\", \"FRUIT1\", \"FVBEANS\", \"FVGREEN\", \"FVORANG\", \"VEGETAB1\", \"EXERANY2\", \"EXRACT11\", \"EXEROFT1\", \"EXERHMM1\", \"EXRACT21\", \"EXEROFT2\", \"EXERHMM2\", \"STRENGTH\", \"LMTJOIN3\", \"ARTHDIS2\", \"ARTHSOCL\", \"JOINPAIN\", \"SEATBELT\", \"FLUSHOT6\", \"FLSHTMY2\", \"IMFVPLAC\", \"PNEUVAC3\", \"HIVTST6\", \"HIVTSTD3\", \"WHRTST10\", \"PDIABTST\", \"PREDIAB1\", \"INSULIN\", \"BLDSUGAR\", \"FEETCHK2\", \"DOCTDIAB\", \"CHKHEMO3\", \"FEETCHK\", \"EYEEXAM\", \"DIABEYE\", \"DIABEDU\", \"CAREGIV1\", \"CRGVREL1\", \"CRGVLNG1\", \"CRGVHRS1\", \"CRGVPRB1\", \"CRGVPERS\", \"CRGVHOUS\", \"CRGVMST2\", \"CRGVEXPT\", \"VIDFCLT2\", \"VIREDIF3\", \"VIPRFVS2\", \"VINOCRE2\", \"VIEYEXM2\", \"VIINSUR2\", \"VICTRCT4\", \"VIGLUMA2\", \"VIMACDG2\", \"CIMEMLOS\", \"CDHOUSE\", \"CDASSIST\", \"CDHELP\", \"CDSOCIAL\", \"CDDISCUS\", \"WTCHSALT\", \"LONGWTCH\", \"DRADVISE\", \"ASTHMAGE\", \"ASATTACK\", \"ASERVIST\", \"ASDRVIST\", \"ASRCHKUP\", \"ASACTLIM\", \"ASYMPTOM\", \"ASNOSLEP\", \"ASTHMED3\", \"ASINHALR\", \"HAREHAB1\", \"STREHAB1\", \"CVDASPRN\", \"ASPUNSAF\", \"RLIVPAIN\", \"RDUCHART\", \"RDUCSTRK\", \"ARTTODAY\", \"ARTHWGT\", \"ARTHEXER\", \"ARTHEDU\", \"TETANUS\", \"HPVADVC2\", \"HPVADSHT\", \"SHINGLE2\", \"HADMAM\", \"HOWLONG\", \"HADPAP2\", \"LASTPAP2\", \"HPVTEST\", \"HPLSTTST\", \"HADHYST2\", \"PROFEXAM\", \"LENGEXAM\", \"BLDSTOOL\", \"LSTBLDS3\", \"HADSIGM3\", \"HADSGCO1\", \"LASTSIG3\", \"PCPSAAD2\", \"PCPSADI1\", \"PCPSARE1\", \"PSATEST1\", \"PSATIME\", \"PCPSARS1\", \"PCPSADE1\", \"PCDMDECN\", \"SCNTMNY1\", \"SCNTMEL1\", \"SCNTPAID\", \"SCNTWRK1\", \"SCNTLPAD\", \"SCNTLWK1\", \"SXORIENT\", \"TRNSGNDR\", \"RCSGENDR\", \"RCSRLTN2\", \"CASTHDX2\", \"CASTHNO2\", \"EMTSUPRT\", \"LSATISFY\", \"ADPLEASR\", \"ADDOWN\", \"ADSLEEP\", \"ADENERGY\", \"ADEAT1\", \"ADFAIL\", \"ADTHINK\", \"ADMOVE\", \"MISTMNT\", \"ADANXEV\", \"QSTVER\", \"QSTLANG\", \"MSCODE\", \"_STSTR\", \"_STRWT\", \"_RAWRAKE\", \"_WT2RAKE\", \"_CHISPNC\", \"_CRACE1\", \"_CPRACE\", \"_CLLCPWT\", \"_DUALUSE\", \"_DUALCOR\", \"_LLCPWT\", \"_RFHLTH\", \"_HCVU651\", \"_RFHYPE5\", \"_CHOLCHK\", \"_RFCHOL\", \"_LTASTH1\", \"_CASTHM1\", \"_ASTHMS1\", \"_DRDXAR1\", \"_PRACE1\", \"_MRACE1\", \"_HISPANC\", \"_RACE\", \"_RACEG21\", \"_RACEGR3\", \"_RACE_G1\", \"_AGEG5YR\", \"_AGE65YR\", \"_AGE80\", \"_AGE_G\", \"HTIN4\", \"HTM4\", \"WTKG3\", \"_BMI5\", \"_BMI5CAT\", \"_RFBMI5\", \"_CHLDCNT\", \"_EDUCAG\", \"_INCOMG\", \"_SMOKER3\", \"_RFSMOK3\", \"DRNKANY5\", \"DROCDY3_\", \"_RFBING5\", \"_DRNKWEK\", \"_RFDRHV5\", \"FTJUDA1_\", \"FRUTDA1_\", \"BEANDAY_\", \"GRENDAY_\", \"ORNGDAY_\", \"VEGEDA1_\", \"_MISFRTN\", \"_MISVEGN\", \"_FRTRESP\", \"_VEGRESP\", \"_FRUTSUM\", \"_VEGESUM\", \"_FRTLT1\", \"_VEGLT1\", \"_FRT16\", \"_VEG23\", \"_FRUITEX\", \"_VEGETEX\", \"_TOTINDA\", \"METVL11_\", \"METVL21_\", \"MAXVO2_\", \"FC60_\", \"ACTIN11_\", \"ACTIN21_\", \"PADUR1_\", \"PADUR2_\", \"PAFREQ1_\", \"PAFREQ2_\", \"_MINAC11\", \"_MINAC21\", \"STRFREQ_\", \"PAMISS1_\", \"PAMIN11_\", \"PAMIN21_\", \"PA1MIN_\", \"PAVIG11_\", \"PAVIG21_\", \"PA1VIGM_\", \"_PACAT1\", \"_PAINDX1\", \"_PA150R2\", \"_PA300R2\", \"_PA30021\", \"_PASTRNG\", \"_PAREC1\", \"_PASTAE1\", \"_LMTACT1\", \"_LMTWRK1\", \"_LMTSCL1\", \"_RFSEAT2\", \"_RFSEAT3\", \"_FLSHOT6\", \"_PNEUMO2\", \"_AIDTST3\"])\n",
    "x_label[indices_to_consider]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "95625c2ec16e567b",
   "metadata": {},
   "source": [
    "```\n",
    "co = categorical/ordinal, n = numerical\n",
    "_STATE: 53 co\n",
    "FMONTH: 12 \n",
    "IMONTH: 12 \n",
    "IDAY: 31 \n",
    "PHYSHLTH: 32 \n",
    "MENTHLTH: 32 \n",
    "POORHLTH: 32 \n",
    "CHILDREN: 11\n",
    "INCOME2: 11\n",
    "HEIGHT3: 57 n\n",
    "ALCDAY5: 38\n",
    "FRUITJU1: 50 \n",
    "FVBEANS: 49\n",
    "FVGREEN: 56 wtf is this format ðŸ˜‚\n",
    "FVORANG: 52\n",
    "STRENGTH: 49\n",
    "_AGEG5YR: 14\n",
    "HTIN4: 32\n",
    "HTM4: 46\n",
    "DROCDY3_: 34\n",
    "FTJUDA1_: 43\n",
    "FRUTDA1_: 59 n\n",
    "BEANDAY_: 42\n",
    "GRENDAY_: 48\n",
    "ORNGDAY_: 43\n",
    "VEGEDA1_: 58 I honestly dont know\n",
    "METVL11_: 29\n",
    "METVL21_: 28\n",
    "PAFREQ1_: 55\n",
    "STRFREQ_: 44\n",
    "```\n",
    "\n",
    "--> The answer was right under our nose ! The co feature with the most distinct values is _STATE."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "After methodical data exploration and analysis, we observed that the co feature that had \n",
    "the most distinct values (53) was _STATE which is our first feature. \n",
    "\n",
    "We consider every feature that has 53 distinct values or less as categorical or ordinal, and every feature that has 54 values or more as numerical.\n",
    " \n",
    "TODO: this is with the NAN THRESHOLD = 50 -> update if we change threshold."
   ],
   "id": "583e61c8b458904c"
  },
  {
   "cell_type": "code",
   "id": "52b96a3e-5bed-4a5a-888b-2cb3b051af1a",
   "metadata": {},
   "source": [
    "THRESHOLD_CO = 53\n",
    "index_of_co_features = np.where(unique_counts_and_indices[:,0] <= THRESHOLD_CO)[0]\n",
    "index_of_numerical_features = np.where(unique_counts_and_indices[:,0] > THRESHOLD_CO)[0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "69b34bfe84ec8a13",
   "metadata": {},
   "source": [
    "Now that we have isolated co features from numerical features, we can\n",
    "- Impute missing values represented by a nan correctly (ex: replace nan with mode if co, replace nan with mean if numerical)\n",
    "- Encode correctly our co features by:\n",
    "    - One hot encoding\n",
    "    - Keeping the original feature to make sure we dont use the ordinality of the ordinal features, as we dont make a difference between ordinal and categorical features.\n",
    "\n",
    "TODO: manual cleaning to get rid of residual nan (for example: 9999 values that still mean nan). Maybe we get rid of them when we get rid of outliers later ?"
   ]
  },
  {
   "cell_type": "code",
   "id": "987b7e1e-5e72-4d88-bc1e-36991b3a8f11",
   "metadata": {},
   "source": [
    "'''\n",
    "Starting here, we assume that every unusable value is a nan.\n",
    "\n",
    "How do we impute missing values ?\n",
    "We can take two routes:\n",
    "1) Univariate imputation of missing values (we only consider the values of the related feature to impute missing values)\n",
    "    - For numerical features: We replace by the mean or the median \n",
    "    - For co features: We replace nan values by a unique value to distinguish them\n",
    "2) Multivariate imputation of missing values (we take into account all of the features to impute missing values)\n",
    "    - Multiple techniques exist (KNN based, regression based...)\n",
    "\n",
    "\n",
    "You have to agree that option 2 has a lot more style (and relevance). But it could also be a pain to implement with numpy \n",
    "(one usually uses scikit learn for this purpose) and as we did not see anything in class, \n",
    "all that we do is extra so we will go the easy route (1) for now. \n",
    "\n",
    "todo can we try 2) Multivariate imputation\n",
    "'''\n",
    "\n",
    "def impute_nan_values(x_tr, x_te, num_indices, co_indices):\n",
    "    # For numerical features: replace nan with median (more robust to outliers)\n",
    "    medians = np.nanmedian(x_tr[:, num_indices], axis=0)\n",
    "    x_tr[:, num_indices] = np.nan_to_num(x_tr[:, num_indices], nan=medians)\n",
    "    x_te[:, num_indices] = np.nan_to_num(x_te[:, num_indices], nan=medians)\n",
    "    \n",
    "    # For categorical/ordinal features: replace nan with maximum + 1\n",
    "    # It acts as a kind of new â€œnan category,â€ which we believe makes\n",
    "    # more sense than taking the featureâ€™s mode\n",
    "    maximums = np.nanmax(x_tr[:, co_indices], axis=0) + 1\n",
    "    x_tr[:, index_of_co_features] = np.nan_to_num(x_tr[:, co_indices], nan=maximums)\n",
    "    x_te[:, index_of_co_features] = np.nan_to_num(x_te[:, co_indices], nan=maximums)\n",
    "    \n",
    "    return x_tr, x_te\n",
    "\n",
    "x_tr, x_te = impute_nan_values(x_tr, x_te, index_of_numerical_features, index_of_co_features)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "56686cd2-7f35-462d-b7a0-dea5e6cc3cb3",
   "metadata": {},
   "source": [
    "'''\n",
    "Feature transformation\n",
    "\n",
    "- For numerical features: We standardize by substracting the mean and dividing by the std (todo we can also normalize them, which is better ? (ex: search min max normalization))\n",
    "- For co features: \n",
    "    1) We add the one hot encoding while also keeping the original feature (to keep relations of ordinality for ordinal features)\n",
    "    2) Can we handle nan values this way ? We need to check the results. (todo give me your opinion) \n",
    "'''\n",
    "\n",
    "def standardize_some_features(x_tr, x_te, features_mask):\n",
    "    means = np.mean(x_tr[:, features_mask], axis=0)\n",
    "    stds = np.std(x_tr[:, features_mask], axis=0)\n",
    "    stds = np.where(stds == 0, 1, stds)\n",
    "    x_tr[:, features_mask] = (x_tr[:, features_mask] - means) / stds\n",
    "    x_te[:, features_mask] = (x_te[:, features_mask] - means) / stds\n",
    "    return x_tr, x_te\n",
    "\n",
    "x_tr, x_te = standardize_some_features(x_tr, x_te, index_of_numerical_features)\n",
    "\n",
    "def encode_categorical_ordinal_features(x_tr, x_te):\n",
    "    merged_x = np.vstack((x_tr, x_te))\n",
    "    result = []\n",
    "    for x in [x_tr, x_te]:\n",
    "        N = len(x)\n",
    "        for idx in index_of_co_features:\n",
    "            unique_values = np.unique(merged_x[:, idx])\n",
    "            one_hot_length = len(unique_values)\n",
    "            one_hot_features = np.zeros((N, one_hot_length)) #todo say dtype is int ?\n",
    "        \n",
    "            # Here the objective is to go from the value to the index in the array of unique values\n",
    "            val_to_index = {value: index for index, value in enumerate(unique_values)}\n",
    "            \n",
    "            for i in range(N):\n",
    "                val = x[i, idx]\n",
    "                val_index = val_to_index[val]\n",
    "                one_hot_features[i, val_index] = 1\n",
    "        \n",
    "            # Add these new features to our x matrix\n",
    "            x = np.hstack((x, one_hot_features))\n",
    "    \n",
    "        print(x.shape)\n",
    "        result.append(x)\n",
    "\n",
    "    # We also standardize the original co features which can have funky values\n",
    "    x_tr, x_te = standardize_some_features(result[0], result[1], index_of_co_features)\n",
    "    \n",
    "    return x_tr, x_te\n",
    "    \n",
    "x_tr, x_te = encode_categorical_ordinal_features(x_tr, x_te)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "57556382-9b36-4a3d-ac76-c999905d0521",
   "metadata": {},
   "source": [
    "# We are now expecting no nan in our dataset whatsoever.\n",
    "assert not np.isnan(x_tr).any()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2f83c0583b286164",
   "metadata": {},
   "source": [
    "\n",
    "that are not possible according to the documentation through boxplots or condition enforcement \n",
    "(schematic example: enforce that weights are inferior to 1000 and if not cap them to the max(1000)) \n",
    "we can also z-score normalization and check outside [-3,3] which indicates outlier (understand method). We need to make it as automatic as possible, this seems like a better idea."
   ]
  },
  {
   "cell_type": "code",
   "id": "3469c155b91635a",
   "metadata": {},
   "source": [
    "'''\n",
    "Handling outliers on numerical features\n",
    "\n",
    "We define percentiles to floor and cap the numerical data in order to clean our data from unexpected outliers.\n",
    "'''\n",
    "\n",
    "def clip_numerical_features(x_tr, x_te, indices_num_feat, floor_p=10, cap_p=90):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x_tr: numpy array of shape (N,D), D is the number of features.\n",
    "        floor_p: percentile on which we floor the numerical features.\n",
    "        cap_p: percentile on which we cap the numerical features.\n",
    "        indices_num_feat: The indices of x_tr numerical features.\n",
    "    Returns:\n",
    "        x_tr: numpy array of shape (N,D), D is the number of features. Outliers have been clipped.\n",
    "        x_te: numpy array of shape (N_test,D), D is the number of features. Outliers have been clipped.\n",
    "    \"\"\"\n",
    "    floors = np.percentile(x_tr[:,indices_num_feat], floor_p, axis=0)\n",
    "    caps = np.percentile(x_tr[:,indices_num_feat], cap_p, axis=0)\n",
    "    x_tr[:, indices_num_feat] = np.clip(x_tr[:, indices_num_feat], floors, caps)\n",
    "    x_te[:, indices_num_feat] = np.clip(x_te[:, indices_num_feat], floors, caps)\n",
    "    return x_tr, x_te\n",
    "\n",
    "x_tr, x_te = clip_numerical_features(x_tr, x_te, index_of_numerical_features)\n",
    "\n",
    "# Outliers have been clipped and there is no Z-score with an absolute value higher than 3\n",
    "assert not (np.abs(x_tr[:,index_of_numerical_features]) > 3).any()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6299af8c-fc3d-46fe-9056-765e307a3e8b",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfc6d77d649358b",
   "metadata": {},
   "source": [
    "Using a correlation matrix, we would capture pairwise correlation relationships but our dataset captures correlations involving the interaction between three or more variables (ex: dates) \n",
    "      \n",
    "\n",
    "We can use another approach: We do a regularized logistic regression, which forces the model to give more weight to highly predictive features, and close to 0 weight to useless features (as the model tries to spare the amount of weight).\n",
    "\n",
    "We train our model using all the features a first time, then compare the absolute values of the weights and get rid of the features that have an associated weight that has a low enough absolute value (they were not deemed predictive during the regularized regression)\n",
    "\n",
    "As we have seen in ADA, the lower the absolute value of a weight, the less impact a change in the related input feature will have on the prediction."
   ]
  },
  {
   "cell_type": "code",
   "id": "62606314-4843-4380-866b-e22a715e9888",
   "metadata": {},
   "source": [
    "def build_tx(x):\n",
    "    return np.c_[np.ones((x.shape[0], 1)), x]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "60118e17-ddf4-4a9d-9b40-adbfcaa9a6bd",
   "metadata": {},
   "source": [
    "def test_model(w, tx_test, y_test):\n",
    "    # Sigmoid gives a value between 0 and 1\n",
    "    test_probs = sigmoid(tx_test @ w)\n",
    "    # We round to the nearest to get our prediction in {0,1}\n",
    "    # (which we will transform later into {-1,1} for submission\n",
    "    test_preds = np.round(test_probs)\n",
    "    test_error_rate = np.count_nonzero(y_test - test_preds) / len(y_test)\n",
    "    test_accuracy = 1 - test_error_rate\n",
    "    test_f1 = compute_f1_score(y_test, test_preds)\n",
    "\n",
    "    return test_f1, test_accuracy"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "de40b2ca24adbda0",
   "metadata": {},
   "source": [
    "def plot_scores_against_features_kept(scores, percent_of_features_kept):\n",
    "    for score_name, score_values in scores.items():\n",
    "        plt.plot(percent_of_features_kept, score_values, marker='o')\n",
    "        plt.xlabel(\"Percentage of features kept\")\n",
    "        plt.ylabel(score_name)\n",
    "        plt.title(f\"{score_name} as we keep more or less features\")\n",
    "        plt.xticks(percent_of_features_kept)\n",
    "        plt.grid(True)\n",
    "        plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "322836e538bc03db",
   "metadata": {},
   "source": [
    "def train_initial_model(y_tr, tx_train, lambda_, max_iter, gamma):\n",
    "    initial_w = np.zeros(tx_train.shape[1])\n",
    "    w, loss = reg_logistic_regression(y_tr, tx_train, lambda_, initial_w, max_iter, gamma)\n",
    "\n",
    "    # Plot the magnitude of the learned weights\n",
    "    # We see that lots of features carry little weight in the predictions\n",
    "    # So we will try removing the least important features\n",
    "    w_abs = np.abs(w)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(range(len(w_abs)), w_abs)\n",
    "    plt.xlabel(\"Index\")\n",
    "    plt.ylabel(\"Value of w abs\")\n",
    "    plt.title(\"Values in vector w\")\n",
    "    plt.show()\n",
    "    \n",
    "    return w_abs"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d90ea717-81cd-4eb1-91c9-f79f2bca2872",
   "metadata": {},
   "source": [
    "# We want to simplify the model, get better performance and reduce overfitting risk,\n",
    "# by removing the features that do not contribute much to the predictions.\n",
    "\n",
    "# So we only select the top P percent of features that contribute the most to the\n",
    "# predictions, judging by the weight they have in the weight vector w we learned with\n",
    "# logistic regression.\n",
    "\n",
    "# The percentile P is the minimum weight value we require to keep P percent of the features.\n",
    "# We select this P methodically by testing a wide range of possible such thresholds.\n",
    "\n",
    "\n",
    "def split_train_data(x_tr, y_tr, train_percentage=0.9):\n",
    "    \"\"\"\n",
    "    We only need this to find the best features to keep.\n",
    "    For the hyperparameter selection we have proper cross-validation.\n",
    "    \"\"\"\n",
    "    random_seed = 0 #TODO: use random seed\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    N = x_tr.shape[0]\n",
    "    indices = np.random.permutation(N)\n",
    "\n",
    "    tr_set_size = int(N * train_percentage)\n",
    "    tr_indices = indices[:tr_set_size]\n",
    "    te_indices = indices[tr_set_size:]\n",
    "\n",
    "    x_tr = x_tr.copy()\n",
    "    X_train, X_test = x_tr[tr_indices], x_tr[te_indices]\n",
    "    y_train, y_test = y_tr[tr_indices], y_tr[te_indices]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def select_most_significant_features():\n",
    "    # Split the data so we can compute scores on a test set for each selection of features\n",
    "    X_train, X_test, y_train, y_test = split_train_data(x_tr, y_tr, train_percentage=0.9)\n",
    "    tx_train = build_tx(X_train)\n",
    "    tx_test = build_tx(X_test)\n",
    "\n",
    "    # We saw that a small lambda is better to allow for more variance in\n",
    "    # weights, which makes picking the most important features easier\n",
    "    max_iter = 100\n",
    "    gamma = 0.5\n",
    "    lambda_ = 0.0005\n",
    "\n",
    "    # Train a first model using all the features\n",
    "    w_abs = train_initial_model(y_train, tx_train, lambda_, max_iter, gamma)\n",
    "\n",
    "    # We want to test keeping between 100% and 5% of the most significant features\n",
    "    # This means having a threshold weight between the 0th and the 95th percentile of weight values\n",
    "    percentiles_to_test = np.arange(0, 100, 5)\n",
    "    threshold_w = [np.percentile(w_abs, p) for p in percentiles_to_test]\n",
    "    scores = {\n",
    "        'Test F1 Score': [],\n",
    "        'Test Accuracy': [],\n",
    "        'Train Loss': []\n",
    "    }\n",
    "    best_f1 = 0.0\n",
    "    best_features_to_keep = None\n",
    "    for t in threshold_w:\n",
    "        features_to_keep = w_abs > t\n",
    "        tx_train_filtered = tx_train[:, features_to_keep]\n",
    "        tx_test_filtered = tx_test[:, features_to_keep]\n",
    "        initial_w = np.zeros(tx_train_filtered.shape[1])\n",
    "    \n",
    "        w, train_loss = reg_logistic_regression(y_train, tx_train_filtered, lambda_, initial_w, max_iter, gamma)\n",
    "    \n",
    "        test_f1, test_accuracy = test_model(w, tx_test_filtered, y_test)\n",
    "    \n",
    "        # Store the results\n",
    "        scores['Test F1 Score'].append(test_f1)\n",
    "        scores['Test Accuracy'].append(test_accuracy)\n",
    "        scores['Train Loss'].append(train_loss)\n",
    "        # Store the feature selection if this gives the best f1 score so far\n",
    "        if test_f1 > best_f1:\n",
    "            best_features_to_keep = features_to_keep\n",
    "\n",
    "    # We kept the top x% of features contributing most to predictions\n",
    "    percent_of_features_kept = 100 - percentiles_to_test\n",
    "    # Plot the result of our tests\n",
    "    plot_scores_against_features_kept(scores, percent_of_features_kept)\n",
    "    # Return the feature selection that gave the best F1 score\n",
    "    return best_features_to_keep\n",
    "\n",
    "best_features_to_keep = select_most_significant_features()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5c1d5615",
   "metadata": {},
   "source": [
    "# 3. Cross validation and model selection"
   ]
  },
  {
   "cell_type": "code",
   "id": "037c2872",
   "metadata": {},
   "source": [
    "lambdas = 5 * 10 ** (-np.arange(5, 9, dtype=np.float64))\n",
    "tx_train = build_tx(x_tr)[:, best_features_to_keep]\n",
    "initial_w = np.zeros(tx_train.shape[1])\n",
    "max_its = 50 * np.arange(1, 21)\n",
    "gammas = [1, 1.3, 1.5]\n",
    "fold_nums = 5\n",
    "# WARNING: Takes a LOT of time (6 min)\n",
    "best_f1, best_hps, scores = cross_validation_for_parameter_selection(\n",
    "    y_tr, tx_train, kind='logistic', fold_nums=fold_nums, verbose=True, lambdas=lambdas, initial_w=initial_w, max_its=max_its, gammas=gammas\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f7518124",
   "metadata": {},
   "source": [
    "print(\"Best f1 score: {:.2f}\\nBest hyperparameters:\\n\\t{}\".format(best_f1, \"\\n\\t\".join([\"{:10s} {}\".format(k, v) for k, v in best_hps.items()])))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d9c29b84",
   "metadata": {},
   "source": [
    "fig, axs = plt.subplots(nrows=len(lambdas), ncols=len(gammas), sharex=True, sharey=True)\n",
    "fig.set_size_inches(14, 12)\n",
    "fig.suptitle(\"Regularized Logistic Regression:\\naccuracy and F1 score for each combination of hyperparameters\", fontsize='xx-large')\n",
    "for ilambda_, lambda_ in enumerate(lambdas):\n",
    "    for igamma, gamma in enumerate(gammas):\n",
    "        ll = scores[ilambda_][igamma]\n",
    "        ax = axs[ilambda_][igamma]\n",
    "        ax.plot(max_its, ll['accuracy'], '.-', label='Accuracy')\n",
    "        ax.plot(max_its, ll['f1'], '.-', label='F1 Score')\n",
    "        ax.set_xticks(max_its, [str(m) if i % 4 == 1 else \"\" for i, m in enumerate(max_its)], rotation='vertical')\n",
    "        ax.set_ybound(-0.05, 1)\n",
    "        yticks = np.linspace(0, 1, 11)\n",
    "        ax.set_yticks(yticks, [\"{:.1f}\".format(x) if i % 2 == 0 else \"\" for i, x in enumerate(yticks)])\n",
    "        if igamma == len(gammas) - 1:\n",
    "            ax.yaxis.set_label_coords(1, 0.5)\n",
    "            ax.set_ylabel(\"lambda = {:.0e}\".format(lambda_), rotation=270, fontsize='large')\n",
    "        if ilambda_ == 0:\n",
    "            ax.set_title(f\"gamma = {gamma}\")\n",
    "            if igamma == len(gammas) - 1:\n",
    "                ax.legend(loc='upper right', ncols=2, bbox_to_anchor=(1, 1.3))\n",
    "        if ilambda_ == len(lambdas) - 1:\n",
    "            ax.set_xlabel(\"Iterations\")\n",
    "fig.savefig('cross_validation')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4. Train the final model and generate predictions for test.csv",
   "id": "343074157161aa0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load the dataset, without sub-sampling\n",
    "x_tr_full_original, x_te_full_original, y_tr_full, tr_id_full, te_id_full = load_csv_data('dataset', sub_sample=False)\n",
    "# Transform our y values from {-1,1} to {0,1} because thatâ€™s what logistic regression tests expect https://edstem.org/eu/courses/1605/discussion/134447\n",
    "y_tr_full[y_tr_full == -1] = 0"
   ],
   "id": "53cdc960f4f9e882",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Make a copy so we can just rerun this cell to start over instead of re-loading the dataset from disk, which is long\n",
    "x_tr_full = x_tr_full_original#.copy()\n",
    "x_te_full = x_te_full_original#.copy()"
   ],
   "id": "7c8dfca625a32c5b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Preprocess the data\n",
    "x_tr_full, x_te_full, _ = filter_features_with_too_many_nans(x_tr_full, x_te_full)\n",
    "x_tr_full, x_te_full = impute_nan_values(x_tr_full, x_te_full, index_of_numerical_features, index_of_co_features)\n",
    "x_tr_full, x_te_full = standardize_some_features(x_tr_full, x_te_full, index_of_numerical_features)\n",
    "x_tr_full, x_te_full = encode_categorical_ordinal_features(x_tr_full, x_te_full)\n",
    "x_tr_full, x_te_full = clip_numerical_features(x_tr_full, x_te_full, index_of_numerical_features)\n",
    "tx_train_full = build_tx(x_tr_full)[:, best_features_to_keep]"
   ],
   "id": "73da5326cf830d02",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Train the final model with the best hyperparameters and the best feature selection\n",
    "initial_w = np.zeros(tx_train_full.shape[1])\n",
    "final_w, final_loss = reg_logistic_regression(y_tr_full, tx_train_full, best_hps.lambda_, initial_w, 100, best_hps.gamma)"
   ],
   "id": "afc6816c94d35ca9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Generate predictions and save them to .csv\n",
    "\n",
    "tx_test_full = build_tx(x_te_full)[:, best_features_to_keep]\n",
    "# Sigmoid gives a value between 0 and 1\n",
    "test_probs_full = sigmoid(tx_test_full @ final_w)\n",
    "# We round to the nearest to get our prediction in {0,1}\n",
    "test_preds_full = np.round(test_probs_full)\n",
    "# Transform them into {-1,1} for submission\n",
    "test_preds_full[test_preds_full == 0] = -1\n",
    "create_csv_submission(te_id_full, test_preds_full, 'full_test_preds.csv')"
   ],
   "id": "3d430d47dd316b2e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
