{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23d8b1c1-ca4c-48a5-97d6-9b4bcce2f45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from helpers import *\n",
    "from implementations import * \n",
    "from utilities.logistic_regression import * #todo specify with the ones we use\n",
    "from utilities.linear_regression import * #todo specify with the ones we use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27630ed0-f71a-490f-84f6-728c96215804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset, sub-sampled\n",
    "x_tr_original, x_te_original, y_tr, tr_id, te_id = load_csv_data('dataset', sub_sample=False)\n",
    "# Transform our y values from {-1,1} to {0,1} because that’s what logistic regression tests expect https://edstem.org/eu/courses/1605/discussion/134447\n",
    "y_tr[y_tr == -1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86f1340504ad4f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy so we can just rerun this cell to start over instead of re-loading the dataset from disk, which is long\n",
    "x_tr = x_tr_original\n",
    "x_te = x_te_original"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6103f79-5c98-4f85-a8ba-9a60a660c1f5",
   "metadata": {},
   "source": [
    "### Data exploration and data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1b7277-ebb0-48c1-b16f-1479637b9eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We observe the data to find any imbalance in the target class representation\n",
    "'''\n",
    "class_imbalance = np.unique(y_tr, return_counts=True)\n",
    "print(class_imbalance)\n",
    "print('Percentage of samples that did not have MICHD: {}'.format(class_imbalance[1][0] / len(y_tr)))\n",
    "print('Percentage of samples that had MICHD: {}'.format(class_imbalance[1][1] / len(y_tr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c999fbf5-08da-42a8-b334-5e2a367aa05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The most important class of samples is highly under-represented. \n",
    "Our predictions should focus on finding True Positives rather than True Negatives, \n",
    "even at the expense of a higher False Positive rate. The reason for that is that the cost\n",
    "of not finding True Positives is very high (not predicting MICHD) while we can afford the \n",
    "cost of some False Positives.\n",
    "\n",
    "To correct this imbalance, we under-sample the majority class (Negative) \n",
    "and over-sample the minority class (Positive) until we reach an equal ratio.\n",
    "\n",
    "'''\n",
    "def balance_dataset(x_tr, y_tr): #todo clean et j'ai utilisé chatGPT, le masquer\n",
    "    '''\n",
    "    Balances the dataset to correct under-representation of the positive minority class (patients that had MICHD)\n",
    "    With M <= N:\n",
    "    Args:\n",
    "        x_tr: numpy array of shape (N,D), D is the number of features.\n",
    "        y_tr: numpy array of shape (N,D), D is the number of features.\n",
    "    Returns:\n",
    "        x_tr_balanced: numpy array of shape (M,D), x_tr with an equal amount of positive and negative class samples\n",
    "        y_tr_balanced: numpy array of shape (M,D), y_tr with an equal amount of positive and negative class samples\n",
    "    '''\n",
    "    \n",
    "    # Masks for negative (majority) and positive (minority) classes\n",
    "    mask_negative = y_tr == 0  \n",
    "    mask_positive = y_tr == 1 \n",
    "    \n",
    "    X_negative, y_negative = x_tr[mask_negative], y_tr[mask_negative]\n",
    "    X_positive, y_positive = x_tr[mask_positive], y_tr[mask_positive]\n",
    "    \n",
    "    num_negative = len(y_negative)\n",
    "    num_positive = len(y_positive)\n",
    "    \n",
    "    #Undersampling the Majority Class todo technique de barbare, on dégage tellement de data... changer\n",
    "    \n",
    "    num_samples_to_keep = len(y_positive) # we keep as many samples as the minority class\n",
    "    \n",
    "    negative_indices = np.random.choice(len(y_negative), num_samples_to_keep, replace=False)\n",
    "    X_negative_undersampled = X_negative[negative_indices]\n",
    "    y_negative_undersampled = y_negative[negative_indices]\n",
    "    \n",
    "    X_balanced = np.vstack([X_negative_undersampled, X_positive])\n",
    "    y_balanced = np.hstack([y_negative_undersampled, y_positive])\n",
    "    \n",
    "    shuffle_indices = np.random.permutation(len(y_balanced))\n",
    "    x_tr_balanced = X_balanced[shuffle_indices]\n",
    "    y_tr_balanced = y_balanced[shuffle_indices]\n",
    "    \n",
    "    print(\"Class distribution after undersampling:\")\n",
    "    print(\"Class 0:\", np.sum(y_tr_balanced == 0))\n",
    "    print(\"Class 1:\", np.sum(y_tr_balanced == 1))\n",
    "    return x_tr_balanced, y_tr_balanced\n",
    "\n",
    "x_tr, y_tr = balance_dataset(x_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f948dda-045a-46eb-be8a-61d96706d5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We analyze the percentage of NaN values in each feature to determine an appropriate cutoff threshold.\n",
    "Features with too many NaN values are less useful, but we want to remove features aimlessly.\n",
    "'''\n",
    "# Plot the nan distribution\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "nan_percentage_by_feature = (np.sum(np.isnan(x_tr), axis=0) / len(x_tr)) * 100\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.hist(nan_percentage_by_feature, bins=100)\n",
    "plt.title(\"Distribution of NaN Percentage by feature\")\n",
    "plt.xlabel(\"NaN Percentage in the feature\")\n",
    "plt.ylabel(\"Number of features\")\n",
    "plt.xticks(np.arange(start=0, stop=101, step=5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45306014-21c1-4841-9700-aa3f63e98ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We analyze the percentage of NaN values in each feature to determine an appropriate cutoff threshold.\n",
    "Features with too many NaN values are less useful, but we don’t want to remove too many features.\n",
    "\n",
    "number of features left for each threshold value:\n",
    "   5% nan: 115 -> 35% of features left\n",
    "  50% nan: 174 -> 54%\n",
    "  65% nan: 194 -> 60%\n",
    "  90% nan: 222 -> 69%\n",
    "we pick a threshold of 50% for now.\n",
    "TODO: test difference in accuracy with different thresholds {5, 50, 65, 90}%\n",
    "\"\"\"\n",
    "\n",
    "MAX_NAN_PERCENTAGE = 50\n",
    "\n",
    "def filter_features_with_too_many_nans(x_tr, x_te, threshold):\n",
    "    # Count the percentage of NaN values per feature\n",
    "    nan_percentage_by_feature = (np.sum(np.isnan(x_tr), axis=0) / len(x_tr)) * 100\n",
    "    \n",
    "    mask = nan_percentage_by_feature < threshold\n",
    "    # So we get rid of the features with more than MAX_NAN_PERCENTAGE % of NaN values\n",
    "    x_tr = x_tr.copy()[:, mask]\n",
    "    x_te = x_te.copy()[:, mask]\n",
    "    from_nan_filtered_to_original_features = np.where(mask)[0]\n",
    "    return x_tr, x_te, from_nan_filtered_to_original_features\n",
    "\n",
    "x_tr, x_te, from_nan_filtered_to_original_features = filter_features_with_too_many_nans(x_tr, x_te, MAX_NAN_PERCENTAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57b2f7e-d332-4d56-8a0d-a6e1f3406648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our objective is to build a mask to separate numerical and co (categorical/ordinal) data. \n",
    "# First we explore the data to see what is the maximum number of unique values for a co feature.\n",
    "\n",
    "# Compute and plot the distribution of unique values \n",
    "unique_counts_and_indices = np.array([[len(np.unique(x_tr[:, i])),i] for i in range(x_tr.shape[1])])\n",
    "unique_counts = unique_counts_and_indices[:,0]\n",
    "\n",
    "plt.hist(unique_counts, bins=50) # Naive visualization, just to have an idea\n",
    "plt.xlabel('Number of Unique Values')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Unique Values Across Features')\n",
    "plt.show()\n",
    "\n",
    "# We consider that a feature with less than 10 values can be considered categorical/ordinal (co),\n",
    "# and one with more than distinct 60 values is surely numerical (think about a human having to\n",
    "# pick 100 options in a survey)\n",
    "\n",
    "# Our goal is to find the co feature with the most distinct values to use it as a threshold\n",
    "unique_counts_and_indices_refined = unique_counts_and_indices[(unique_counts_and_indices[:,0] > 10) & (unique_counts_and_indices[:,0] < 60)]\n",
    "# We get back the original indexes of our x_tr to link our findings to the original feature indices\n",
    "indices_to_consider = from_nan_filtered_to_original_features[unique_counts_and_indices_refined[:,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd63db6-9b1f-4f02-bb9c-e46a1518a7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_label = np.array([\"_STATE\", \"FMONTH\", \"IDATE\", \"IMONTH\", \"IDAY\", \"IYEAR\", \"DISPCODE\", \"SEQNO\", \"_PSU\", \"CTELENUM\", \"PVTRESD1\", \"COLGHOUS\", \"STATERES\", \"CELLFON3\", \"LADULT\", \"NUMADULT\", \"NUMMEN\", \"NUMWOMEN\", \"CTELNUM1\", \"CELLFON2\", \"CADULT\", \"PVTRESD2\", \"CCLGHOUS\", \"CSTATE\", \"LANDLINE\", \"HHADULT\", \"GENHLTH\", \"PHYSHLTH\", \"MENTHLTH\", \"POORHLTH\", \"HLTHPLN1\", \"PERSDOC2\", \"MEDCOST\", \"CHECKUP1\", \"BPHIGH4\", \"BPMEDS\", \"BLOODCHO\", \"CHOLCHK\", \"TOLDHI2\", \"CVDSTRK3\", \"ASTHMA3\", \"ASTHNOW\", \"CHCSCNCR\", \"CHCOCNCR\", \"CHCCOPD1\", \"HAVARTH3\", \"ADDEPEV2\", \"CHCKIDNY\", \"DIABETE3\", \"DIABAGE2\", \"SEX\", \"MARITAL\", \"EDUCA\", \"RENTHOM1\", \"NUMHHOL2\", \"NUMPHON2\", \"CPDEMO1\", \"VETERAN3\", \"EMPLOY1\", \"CHILDREN\", \"INCOME2\", \"INTERNET\", \"WEIGHT2\", \"HEIGHT3\", \"PREGNANT\", \"QLACTLM2\", \"USEEQUIP\", \"BLIND\", \"DECIDE\", \"DIFFWALK\", \"DIFFDRES\", \"DIFFALON\", \"SMOKE100\", \"SMOKDAY2\", \"STOPSMK2\", \"LASTSMK2\", \"USENOW3\", \"ALCDAY5\", \"AVEDRNK2\", \"DRNK3GE5\", \"MAXDRNKS\", \"FRUITJU1\", \"FRUIT1\", \"FVBEANS\", \"FVGREEN\", \"FVORANG\", \"VEGETAB1\", \"EXERANY2\", \"EXRACT11\", \"EXEROFT1\", \"EXERHMM1\", \"EXRACT21\", \"EXEROFT2\", \"EXERHMM2\", \"STRENGTH\", \"LMTJOIN3\", \"ARTHDIS2\", \"ARTHSOCL\", \"JOINPAIN\", \"SEATBELT\", \"FLUSHOT6\", \"FLSHTMY2\", \"IMFVPLAC\", \"PNEUVAC3\", \"HIVTST6\", \"HIVTSTD3\", \"WHRTST10\", \"PDIABTST\", \"PREDIAB1\", \"INSULIN\", \"BLDSUGAR\", \"FEETCHK2\", \"DOCTDIAB\", \"CHKHEMO3\", \"FEETCHK\", \"EYEEXAM\", \"DIABEYE\", \"DIABEDU\", \"CAREGIV1\", \"CRGVREL1\", \"CRGVLNG1\", \"CRGVHRS1\", \"CRGVPRB1\", \"CRGVPERS\", \"CRGVHOUS\", \"CRGVMST2\", \"CRGVEXPT\", \"VIDFCLT2\", \"VIREDIF3\", \"VIPRFVS2\", \"VINOCRE2\", \"VIEYEXM2\", \"VIINSUR2\", \"VICTRCT4\", \"VIGLUMA2\", \"VIMACDG2\", \"CIMEMLOS\", \"CDHOUSE\", \"CDASSIST\", \"CDHELP\", \"CDSOCIAL\", \"CDDISCUS\", \"WTCHSALT\", \"LONGWTCH\", \"DRADVISE\", \"ASTHMAGE\", \"ASATTACK\", \"ASERVIST\", \"ASDRVIST\", \"ASRCHKUP\", \"ASACTLIM\", \"ASYMPTOM\", \"ASNOSLEP\", \"ASTHMED3\", \"ASINHALR\", \"HAREHAB1\", \"STREHAB1\", \"CVDASPRN\", \"ASPUNSAF\", \"RLIVPAIN\", \"RDUCHART\", \"RDUCSTRK\", \"ARTTODAY\", \"ARTHWGT\", \"ARTHEXER\", \"ARTHEDU\", \"TETANUS\", \"HPVADVC2\", \"HPVADSHT\", \"SHINGLE2\", \"HADMAM\", \"HOWLONG\", \"HADPAP2\", \"LASTPAP2\", \"HPVTEST\", \"HPLSTTST\", \"HADHYST2\", \"PROFEXAM\", \"LENGEXAM\", \"BLDSTOOL\", \"LSTBLDS3\", \"HADSIGM3\", \"HADSGCO1\", \"LASTSIG3\", \"PCPSAAD2\", \"PCPSADI1\", \"PCPSARE1\", \"PSATEST1\", \"PSATIME\", \"PCPSARS1\", \"PCPSADE1\", \"PCDMDECN\", \"SCNTMNY1\", \"SCNTMEL1\", \"SCNTPAID\", \"SCNTWRK1\", \"SCNTLPAD\", \"SCNTLWK1\", \"SXORIENT\", \"TRNSGNDR\", \"RCSGENDR\", \"RCSRLTN2\", \"CASTHDX2\", \"CASTHNO2\", \"EMTSUPRT\", \"LSATISFY\", \"ADPLEASR\", \"ADDOWN\", \"ADSLEEP\", \"ADENERGY\", \"ADEAT1\", \"ADFAIL\", \"ADTHINK\", \"ADMOVE\", \"MISTMNT\", \"ADANXEV\", \"QSTVER\", \"QSTLANG\", \"MSCODE\", \"_STSTR\", \"_STRWT\", \"_RAWRAKE\", \"_WT2RAKE\", \"_CHISPNC\", \"_CRACE1\", \"_CPRACE\", \"_CLLCPWT\", \"_DUALUSE\", \"_DUALCOR\", \"_LLCPWT\", \"_RFHLTH\", \"_HCVU651\", \"_RFHYPE5\", \"_CHOLCHK\", \"_RFCHOL\", \"_LTASTH1\", \"_CASTHM1\", \"_ASTHMS1\", \"_DRDXAR1\", \"_PRACE1\", \"_MRACE1\", \"_HISPANC\", \"_RACE\", \"_RACEG21\", \"_RACEGR3\", \"_RACE_G1\", \"_AGEG5YR\", \"_AGE65YR\", \"_AGE80\", \"_AGE_G\", \"HTIN4\", \"HTM4\", \"WTKG3\", \"_BMI5\", \"_BMI5CAT\", \"_RFBMI5\", \"_CHLDCNT\", \"_EDUCAG\", \"_INCOMG\", \"_SMOKER3\", \"_RFSMOK3\", \"DRNKANY5\", \"DROCDY3_\", \"_RFBING5\", \"_DRNKWEK\", \"_RFDRHV5\", \"FTJUDA1_\", \"FRUTDA1_\", \"BEANDAY_\", \"GRENDAY_\", \"ORNGDAY_\", \"VEGEDA1_\", \"_MISFRTN\", \"_MISVEGN\", \"_FRTRESP\", \"_VEGRESP\", \"_FRUTSUM\", \"_VEGESUM\", \"_FRTLT1\", \"_VEGLT1\", \"_FRT16\", \"_VEG23\", \"_FRUITEX\", \"_VEGETEX\", \"_TOTINDA\", \"METVL11_\", \"METVL21_\", \"MAXVO2_\", \"FC60_\", \"ACTIN11_\", \"ACTIN21_\", \"PADUR1_\", \"PADUR2_\", \"PAFREQ1_\", \"PAFREQ2_\", \"_MINAC11\", \"_MINAC21\", \"STRFREQ_\", \"PAMISS1_\", \"PAMIN11_\", \"PAMIN21_\", \"PA1MIN_\", \"PAVIG11_\", \"PAVIG21_\", \"PA1VIGM_\", \"_PACAT1\", \"_PAINDX1\", \"_PA150R2\", \"_PA300R2\", \"_PA30021\", \"_PASTRNG\", \"_PAREC1\", \"_PASTAE1\", \"_LMTACT1\", \"_LMTWRK1\", \"_LMTSCL1\", \"_RFSEAT2\", \"_RFSEAT3\", \"_FLSHOT6\", \"_PNEUMO2\", \"_AIDTST3\"])\n",
    "x_label[indices_to_consider]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95625c2ec16e567b",
   "metadata": {},
   "source": [
    "```\n",
    "co = categorical/ordinal, n = numerical\n",
    "_STATE: 53 co\n",
    "FMONTH: 12 \n",
    "IMONTH: 12 \n",
    "IDAY: 31 \n",
    "PHYSHLTH: 32 \n",
    "MENTHLTH: 32 \n",
    "POORHLTH: 32 \n",
    "CHILDREN: 11\n",
    "INCOME2: 11\n",
    "HEIGHT3: 57 n\n",
    "ALCDAY5: 38\n",
    "FRUITJU1: 50 \n",
    "FVBEANS: 49\n",
    "FVGREEN: 56 wtf is this format 😂\n",
    "FVORANG: 52\n",
    "STRENGTH: 49\n",
    "_AGEG5YR: 14\n",
    "HTIN4: 32\n",
    "HTM4: 46\n",
    "DROCDY3_: 34\n",
    "FTJUDA1_: 43\n",
    "FRUTDA1_: 59 n\n",
    "BEANDAY_: 42\n",
    "GRENDAY_: 48\n",
    "ORNGDAY_: 43\n",
    "VEGEDA1_: 58 I honestly dont know\n",
    "METVL11_: 29\n",
    "METVL21_: 28\n",
    "PAFREQ1_: 55\n",
    "STRFREQ_: 44\n",
    "```\n",
    "\n",
    "--> The answer was right under our nose ! The co feature with the most distinct values is _STATE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52b96a3e-5bed-4a5a-888b-2cb3b051af1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "After methodical data exploration and analysis, we observed that the co feature that had \n",
    "the most distinct values (53) was _STATE which is our first feature. \n",
    "\n",
    "We consider every feature that has 53 distinct values or less as categorical or ordinal, and every feature that has 54 values or more as numerical.\n",
    "'''\n",
    "THRESHOLD_CO = 53\n",
    "index_of_co_features = np.where(unique_counts_and_indices[:,0] <= THRESHOLD_CO)[0]\n",
    "index_of_numerical_features = np.where(unique_counts_and_indices[:,0] > THRESHOLD_CO)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b34bfe84ec8a13",
   "metadata": {},
   "source": [
    "Now that we have isolated co features from numerical features, we can\n",
    "- Impute missing values represented by a nan correctly (ex: replace nan with mode if co, replace nan with mean if numerical)\n",
    "- Encode correctly our co features by:\n",
    "    - One hot encoding\n",
    "    - Keeping the original feature to make sure we dont use the ordinality of the ordinal features, as we dont make a difference between ordinal and categorical features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "987b7e1e-5e72-4d88-bc1e-36991b3a8f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Starting here, we assume that every unusable value is a nan.\n",
    "\n",
    "How do we impute missing values ?\n",
    "We can take two routes:\n",
    "1) Univariate imputation of missing values (we only consider the values of the related feature to impute missing values)\n",
    "    - For numerical features: We replace by the mean or the median \n",
    "    - For co features: We replace nan values by a unique value to distinguish them\n",
    "2) Multivariate imputation of missing values (we take into account all of the features to impute missing values)\n",
    "    - Multiple techniques exist (KNN based, regression based...)\n",
    "\n",
    "\n",
    "You have to agree that option 2 has a lot more style (and relevance). But it could also be a pain to implement with numpy \n",
    "(one usually uses scikit learn for this purpose) and as we did not see anything in class, \n",
    "all that we do is extra so we will go the easy route (1) for now. \n",
    "\n",
    "todo can we try 2) Multivariate imputation\n",
    "'''\n",
    "\n",
    "def impute_nan_values(x_tr, x_te, num_indices, co_indices):\n",
    "    # For numerical features: replace nan with median (more robust to outliers)\n",
    "    medians = np.nanmedian(x_tr[:, num_indices], axis=0)\n",
    "    x_tr[:, num_indices] = np.nan_to_num(x_tr[:, num_indices], nan=medians)\n",
    "    x_te[:, num_indices] = np.nan_to_num(x_te[:, num_indices], nan=medians)\n",
    "    \n",
    "    # For categorical/ordinal features: replace nan with maximum + 1\n",
    "    # It acts as a kind of new “nan category,” which we believe makes\n",
    "    # more sense than taking the feature’s mode\n",
    "    maximums = np.nanmax(x_tr[:, co_indices], axis=0) + 1\n",
    "    x_tr[:, index_of_co_features] = np.nan_to_num(x_tr[:, co_indices], nan=maximums)\n",
    "    x_te[:, index_of_co_features] = np.nan_to_num(x_te[:, co_indices], nan=maximums)\n",
    "    \n",
    "    return x_tr, x_te\n",
    "\n",
    "x_tr, x_te = impute_nan_values(x_tr, x_te, index_of_numerical_features, index_of_co_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56686cd2-7f35-462d-b7a0-dea5e6cc3cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Feature transformation\n",
    "\n",
    "- For numerical features: We standardize by substracting the mean and dividing by the std (todo we can also normalize them, which is better ? (ex: search min max normalization))\n",
    "- For co features: \n",
    "    1) We add the one hot encoding while also keeping the original feature (to keep relations of ordinality for ordinal features)\n",
    "    2) Can we handle nan values this way ? We need to check the results. (todo give me your opinion) \n",
    "'''\n",
    "\n",
    "#todo add doc to every function\n",
    "def standardize_some_features(x_tr, x_te, features_mask): #todo change name\n",
    "    means = np.mean(x_tr[:, features_mask], axis=0)\n",
    "    stds = np.std(x_tr[:, features_mask], axis=0)\n",
    "    stds = np.where(stds == 0, 1, stds)\n",
    "    x_tr[:, features_mask] = (x_tr[:, features_mask] - means) / stds\n",
    "    x_te[:, features_mask] = (x_te[:, features_mask] - means) / stds\n",
    "    return x_tr, x_te\n",
    "\n",
    "x_tr, x_te = standardize_some_features(x_tr, x_te, index_of_numerical_features)\n",
    "\n",
    "def encode_categorical_ordinal_features(x_tr, x_te):\n",
    "    merged_x = np.vstack((x_tr, x_te))\n",
    "    result = []\n",
    "    for x in [x_tr, x_te]:\n",
    "        N = len(x)\n",
    "        for idx in index_of_co_features:\n",
    "            unique_values = np.unique(merged_x[:, idx])\n",
    "            one_hot_length = len(unique_values)\n",
    "            one_hot_features = np.zeros((N, one_hot_length)) #todo say dtype is int ?\n",
    "        \n",
    "            # Here the objective is to go from the value to the index in the array of unique values\n",
    "            val_to_index = {value: index for index, value in enumerate(unique_values)}\n",
    "            \n",
    "            for i in range(N):\n",
    "                val = x[i, idx]\n",
    "                val_index = val_to_index[val]\n",
    "                one_hot_features[i, val_index] = 1\n",
    "        \n",
    "            # Add these new features to our x matrix\n",
    "            x = np.hstack((x, one_hot_features))\n",
    "    \n",
    "        result.append(x)\n",
    "\n",
    "    # We also standardize the original co features which can have funky values\n",
    "    x_tr, x_te = standardize_some_features(result[0], result[1], index_of_co_features)\n",
    "    \n",
    "    return x_tr, x_te\n",
    "    \n",
    "x_tr, x_te = encode_categorical_ordinal_features(x_tr, x_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57556382-9b36-4a3d-ac76-c999905d0521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are now expecting no nan in our dataset whatsoever.\n",
    "assert not np.isnan(x_tr).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a567a48-1e05-4ceb-ac3f-70ebbf12be5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Detecting outliers on numerical features\n",
    "\n",
    "We have already standardized the numerical features, hence their value is their Z-score that we do not have to compute\n",
    "again. First, we analyse the values of the numerical components of the samples and we define a Z-score threshold of 3\n",
    "beyond which they are considered as outliers. We now check for the presence of outliers in our numerical features.\n",
    "'''\n",
    "\n",
    "(np.abs(x_tr[:,index_of_numerical_features]) > 3).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3469c155b91635a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Handling outliers on numerical features\n",
    "\n",
    "We define percentiles to floor and cap the numerical data in order to clean our data from unexpected outliers.\n",
    "'''\n",
    "\n",
    "def clip_numerical_features(x_tr, x_te, indices_num_feat, floor_p=10, cap_p=90):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x_tr: numpy array of shape (N,D), D is the number of features.\n",
    "        floor_p: percentile on which we floor the numerical features.\n",
    "        cap_p: percentile on which we cap the numerical features.\n",
    "        indices_num_feat: The indices of x_tr numerical features.\n",
    "    Returns:\n",
    "        x_tr: numpy array of shape (N,D), D is the number of features. Outliers have been clipped.\n",
    "        x_te: numpy array of shape (N_test,D), D is the number of features. Outliers have been clipped.\n",
    "    \"\"\"\n",
    "    floors = np.percentile(x_tr[:,indices_num_feat], floor_p, axis=0)\n",
    "    caps = np.percentile(x_tr[:,indices_num_feat], cap_p, axis=0)\n",
    "    x_tr[:, indices_num_feat] = np.clip(x_tr[:, indices_num_feat], floors, caps)\n",
    "    x_te[:, indices_num_feat] = np.clip(x_te[:, indices_num_feat], floors, caps)\n",
    "    return x_tr, x_te\n",
    "\n",
    "x_tr, x_te = clip_numerical_features(x_tr, x_te, index_of_numerical_features)\n",
    "\n",
    "# Outliers have been clipped and there is no Z-score with an absolute value higher than 3\n",
    "assert not (np.abs(x_tr[:,index_of_numerical_features]) > 3).any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6299af8c-fc3d-46fe-9056-765e307a3e8b",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfc6d77d649358b",
   "metadata": {},
   "source": [
    "Using a correlation matrix, we would capture pairwise correlation relationships but our dataset captures correlations involving the interaction between three or more variables (ex: dates) \n",
    "      \n",
    "\n",
    "We can use another approach: We do a regularized logistic regression, which forces the model to give more weight to highly predictive features, and close to 0 weight to useless features (as the model tries to spare the amount of weight).\n",
    "\n",
    "We train our model using all the features a first time, then compare the absolute values of the weights and get rid of the features that have an associated weight that has a low enough absolute value (they were not deemed predictive during the regularized regression)\n",
    "\n",
    "As we have seen in ADA, the lower the absolute value of a weight, the less impact a change in the related input feature will have on the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62606314-4843-4380-866b-e22a715e9888",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tx(x):\n",
    "    return np.c_[np.ones((x.shape[0], 1)), x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60118e17-ddf4-4a9d-9b40-adbfcaa9a6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(w, tx_test, y_test):\n",
    "    # Sigmoid gives a value between 0 and 1\n",
    "    test_probs = sigmoid(tx_test @ w)\n",
    "    # We round to the nearest to get our prediction in {0,1}\n",
    "    # (which we will transform later into {-1,1} for submission\n",
    "    test_preds = np.round(test_probs)\n",
    "    test_error_rate = np.count_nonzero(y_test - test_preds) / len(y_test)\n",
    "    test_accuracy = 1 - test_error_rate\n",
    "    test_f1 = compute_f1_score(y_test, test_preds)\n",
    "\n",
    "    return test_f1, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de40b2ca24adbda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scores_against_features_kept(scores, percent_of_features_kept):\n",
    "    for score_name, score_values in scores.items():\n",
    "        plt.plot(percent_of_features_kept, score_values, marker='o')\n",
    "        plt.xlabel(\"Percentage of features kept\")\n",
    "        plt.ylabel(score_name)\n",
    "        plt.title(f\"{score_name} as we keep more or less features\")\n",
    "        plt.xticks(percent_of_features_kept)\n",
    "        plt.grid(True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "322836e538bc03db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_initial_model(y_tr, tx_train, lambda_, max_iter, gamma):\n",
    "    initial_w = np.zeros(tx_train.shape[1])\n",
    "    w, loss = reg_logistic_regression(y_tr, tx_train, lambda_, initial_w, max_iter, gamma)\n",
    "\n",
    "    # Plot the magnitude of the learned weights\n",
    "    # We see that lots of features carry little weight in the predictions\n",
    "    # So we will try removing the least important features\n",
    "    w_abs = np.abs(w)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(range(len(w_abs)), w_abs)\n",
    "    plt.xlabel(\"Index\")\n",
    "    plt.ylabel(\"Value of w abs\")\n",
    "    plt.title(\"Values in vector w\")\n",
    "    plt.show()\n",
    "    \n",
    "    return w_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90ea717-81cd-4eb1-91c9-f79f2bca2872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to simplify the model, get better performance and reduce overfitting risk,\n",
    "# by removing the features that do not contribute much to the predictions.\n",
    "\n",
    "# So we only select the top P percent of features that contribute the most to the\n",
    "# predictions, judging by the weight they have in the weight vector w we learned with\n",
    "# logistic regression.\n",
    "\n",
    "# The percentile P is the minimum weight value we require to keep P percent of the features.\n",
    "# We select this P methodically by testing a wide range of possible such thresholds.\n",
    "\n",
    "\n",
    "def split_train_data(x_tr, y_tr, train_percentage=0.9):\n",
    "    \"\"\"\n",
    "    We only need this to find the best features to keep.\n",
    "    For the hyperparameter selection we have proper cross-validation.\n",
    "    \"\"\"\n",
    "    random_seed = 0 #TODO: use random seed\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    N = x_tr.shape[0]\n",
    "    indices = np.random.permutation(N)\n",
    "\n",
    "    tr_set_size = int(N * train_percentage)\n",
    "    tr_indices = indices[:tr_set_size]\n",
    "    te_indices = indices[tr_set_size:]\n",
    "\n",
    "    x_tr = x_tr.copy()\n",
    "    X_train, X_test = x_tr[tr_indices], x_tr[te_indices]\n",
    "    y_train, y_test = y_tr[tr_indices], y_tr[te_indices]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def select_most_significant_features():\n",
    "    # Split the data so we can compute scores on a test set for each selection of features\n",
    "    X_train, X_test, y_train, y_test = split_train_data(x_tr, y_tr, train_percentage=0.9)\n",
    "    tx_train = build_tx(X_train)\n",
    "    tx_test = build_tx(X_test)\n",
    "\n",
    "    # We saw that a small lambda is better to allow for more variance in\n",
    "    # weights, which makes picking the most important features easier\n",
    "    max_iter = 100\n",
    "    gamma = 0.5\n",
    "    lambda_ = 0.0005\n",
    "\n",
    "    # Train a first model using all the features\n",
    "    w_abs = train_initial_model(y_train, tx_train, lambda_, max_iter, gamma)\n",
    "\n",
    "    # We want to test keeping between 100% and 5% of the most significant features\n",
    "    # This means having a threshold weight between the 0th and the 95th percentile of weight values\n",
    "    percentiles_to_test = np.arange(0, 100, 5)\n",
    "    threshold_w = [np.percentile(w_abs, p) for p in percentiles_to_test]\n",
    "    scores = {\n",
    "        'Test F1 Score': [],\n",
    "        'Test Accuracy': [],\n",
    "        'Train Loss': []\n",
    "    }\n",
    "    best_f1 = 0.0\n",
    "    best_features_to_keep = None\n",
    "    for t in threshold_w:\n",
    "        features_to_keep = w_abs > t\n",
    "        tx_train_filtered = tx_train[:, features_to_keep]\n",
    "        tx_test_filtered = tx_test[:, features_to_keep]\n",
    "        initial_w = np.zeros(tx_train_filtered.shape[1])\n",
    "    \n",
    "        w, train_loss = reg_logistic_regression(y_train, tx_train_filtered, lambda_, initial_w, max_iter, gamma)\n",
    "    \n",
    "        test_f1, test_accuracy = test_model(w, tx_test_filtered, y_test)\n",
    "    \n",
    "        # Store the results\n",
    "        scores['Test F1 Score'].append(test_f1)\n",
    "        scores['Test Accuracy'].append(test_accuracy)\n",
    "        scores['Train Loss'].append(train_loss)\n",
    "        # Store the feature selection if this gives the best f1 score so far\n",
    "        if test_f1 > best_f1:\n",
    "            best_features_to_keep = features_to_keep\n",
    "\n",
    "    # We kept the top x% of features contributing most to predictions\n",
    "    percent_of_features_kept = 100 - percentiles_to_test\n",
    "    # Plot the result of our tests\n",
    "    plot_scores_against_features_kept(scores, percent_of_features_kept)\n",
    "    # Return the feature selection that gave the best F1 score\n",
    "    return best_features_to_keep\n",
    "\n",
    "best_features_to_keep = select_most_significant_features()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c4beaa-1b0c-4729-81b5-aa61c514442e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_features_to_keep.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1d5615",
   "metadata": {},
   "source": [
    "# 3. Cross validation and model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c7330ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.cross_validation import cross_validation_for_model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c0e3d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_nums = 5\n",
    "tx_train = build_tx(x_tr)[:, best_features_to_keep]\n",
    "D = tx_train.shape[1]\n",
    "models = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578aa9e4",
   "metadata": {},
   "source": [
    "## 3.1. Hyperparameter instantiation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5895185",
   "metadata": {},
   "source": [
    "### Regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "037c2872",
   "metadata": {},
   "outputs": [],
   "source": [
    "models += [dict(\n",
    "    name='Regularized Logistic Regression',\n",
    "    kind='logistic',\n",
    "    parameters=dict(\n",
    "        lambdas = 5 * 10 ** (-np.arange(5, 9, dtype=np.float64)),\n",
    "        initial_w = np.zeros(D),\n",
    "        max_its = 50 * np.arange(1, 21),\n",
    "        gammas = [1, 1.3, 1.5]\n",
    "    )\n",
    ")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27f8e60",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca699c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "models += [dict(\n",
    "    name='Logistic Regression',\n",
    "    kind='logistic',\n",
    "    parameters=dict(\n",
    "        initial_w = np.zeros(D),\n",
    "        max_its = 50 * np.arange(1, 21),\n",
    "        gammas=[0.5, 1, 1.5]\n",
    "    )\n",
    ")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8fcdea",
   "metadata": {},
   "source": [
    "### Least Squares Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dc406c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "models += [dict(\n",
    "    name='Least Squares Linear Regression',\n",
    "    kind='least_squares',\n",
    "    parameters=dict()\n",
    ")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575e2bed",
   "metadata": {},
   "source": [
    "### Least Squares Regularized Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "69f30450",
   "metadata": {},
   "outputs": [],
   "source": [
    "models += [dict(\n",
    "    name='Least Squares Regularized Linear Regression',\n",
    "    kind='least_squares',\n",
    "    parameters=dict(\n",
    "        lambdas=5 * 10 ** np.arange(5,10)\n",
    "    )\n",
    ")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92a3cc8",
   "metadata": {},
   "source": [
    "### MSE Linear Regression with GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b3616115",
   "metadata": {},
   "outputs": [],
   "source": [
    "models += [dict(\n",
    "    name='MSE Linear Regression with GD',\n",
    "    kind='linear',\n",
    "    parameters=dict(\n",
    "        initial_w = np.zeros(D),\n",
    "        max_its = 50 * np.arange(1, 21),\n",
    "        gammas = [0.5, 0.7, 1, 1.3, 1.5]\n",
    "    )\n",
    ")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c3d0de",
   "metadata": {},
   "source": [
    "## 3.2. Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddef46ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model, scores_by_model = cross_validation_for_model_selection(y_tr, tx_train, fold_nums=fold_nums, models=models, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7518124",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_f1, best_hps, scores = scores_by_model[best_model] \n",
    "print(\"Best model is {:s} with an f1 score of {:.2f}\\nBest hyperparameters:\\n\\t{}\".format(best_model, best_f1, \"\\n\\t\".join([\"{:10s} {}\".format(k, v) for k, v in best_hps.items()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55a7811",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set_title(\"Best F1 score by model\")\n",
    "ax.set_ylabel(\"F1 Score\")\n",
    "ax.set_xlabel(\"Model Name\")\n",
    "width = 0.25\n",
    "x = np.arange(len(scores_by_model))\n",
    "ax.bar(x=x, height=[f1 for f1, _, _ in scores_by_model.values()], width=width, label=\"F1 score\")\n",
    "ax.set_xticks(x, [name.replace(\" \", \"\\n\") for name in scores_by_model.keys()], fontsize='x-small')\n",
    "ax.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d9c29b84",
   "metadata": {},
   "source": [
    "fig, axs = plt.subplots(nrows=len(lambdas), ncols=len(gammas), sharex=True, sharey=True)\n",
    "fig.set_size_inches(14, 12)\n",
    "fig.suptitle(\"Regularized Logistic Regression:\\naccuracy and F1 score for each combination of hyperparameters\", fontsize='xx-large')\n",
    "for ilambda_, lambda_ in enumerate(lambdas):\n",
    "    for igamma, gamma in enumerate(gammas):\n",
    "        ll = scores[ilambda_][igamma]\n",
    "        ax = axs[ilambda_][igamma]\n",
    "        ax.plot(max_its, ll['accuracy'], '.-', label='Accuracy')\n",
    "        ax.plot(max_its, ll['f1'], '.-', label='F1 Score')\n",
    "        ax.set_xticks(max_its, [str(m) if i % 4 == 1 else \"\" for i, m in enumerate(max_its)], rotation='vertical')\n",
    "        ax.set_ybound(-0.05, 1)\n",
    "        yticks = np.linspace(0, 1, 11)\n",
    "        ax.set_yticks(yticks, [\"{:.1f}\".format(x) if i % 2 == 0 else \"\" for i, x in enumerate(yticks)])\n",
    "        if igamma == len(gammas) - 1:\n",
    "            ax.yaxis.set_label_coords(1, 0.5)\n",
    "            ax.set_ylabel(\"lambda = {:.0e}\".format(lambda_), rotation=270, fontsize='large')\n",
    "        if ilambda_ == 0:\n",
    "            ax.set_title(f\"gamma = {gamma}\")\n",
    "            if igamma == len(gammas) - 1:\n",
    "                ax.legend(loc='upper right', ncols=2, bbox_to_anchor=(1, 1.3))\n",
    "        if ilambda_ == len(lambdas) - 1:\n",
    "            ax.set_xlabel(\"Iterations\")\n",
    "fig.savefig('cross_validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343074157161aa0",
   "metadata": {},
   "source": [
    "# 4. Train the final model and generate predictions for test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "53cdc960f4f9e882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset, without sub-sampling\n",
    "x_tr_full_original, x_te_full_original, y_tr_full_original, tr_id_full, te_id_full = load_csv_data('dataset', sub_sample=False)\n",
    "# Transform our y values from {-1,1} to {0,1} because that’s what logistic regression tests expect https://edstem.org/eu/courses/1605/discussion/134447\n",
    "y_tr_full_original[y_tr_full_original == -1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8dfca625a32c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy so we can just rerun this cell to start over instead of re-loading the dataset from disk, which is long\n",
    "# rebalance the dataset\n",
    "x_tr_full, y_tr_full = balance_dataset(x_tr_full_original, y_tr_full_original)\n",
    "x_te_full = x_te_full_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73da5326cf830d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "x_tr_full, x_te_full, _ = filter_features_with_too_many_nans(x_tr_full, x_te_full, MAX_NAN_PERCENTAGE)\n",
    "x_tr_full, x_te_full = impute_nan_values(x_tr_full, x_te_full, index_of_numerical_features, index_of_co_features)\n",
    "x_tr_full, x_te_full = standardize_some_features(x_tr_full, x_te_full, index_of_numerical_features)\n",
    "x_tr_full, x_te_full = encode_categorical_ordinal_features(x_tr_full, x_te_full)\n",
    "x_tr_full, x_te_full = clip_numerical_features(x_tr_full, x_te_full, index_of_numerical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2eba9e-af58-4be8-b048-00e46939f917",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_features_to_keep.shape)\n",
    "print(x_tr_full.shape)\n",
    "tx_train_full = build_tx(x_tr_full)[:, best_features_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc6816c94d35ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the final model with the best hyperparameters and the best feature selection\n",
    "initial_w = np.zeros(tx_train_full.shape[1])\n",
    "final_w, final_loss = reg_logistic_regression(y_tr_full, tx_train_full, best_hps.lambda_, initial_w, 100, best_hps.gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d430d47dd316b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions and save them to .csv\n",
    "# \n",
    "tx_test_full = build_tx(x_te_full)[:, best_features_to_keep]\n",
    "# # Sigmoid gives a value between 0 and 1\n",
    "test_probs_full = sigmoid(tx_test_full @ final_w)\n",
    "# # We round to the nearest to get our prediction in {0,1}\n",
    "test_preds_full = np.round(test_probs_full)\n",
    "# # Transform them into {-1,1} for submission\n",
    "test_preds_full[test_preds_full == 0] = -1\n",
    "create_csv_submission(te_id_full, test_preds_full, 'full_test_preds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9422ed3f76831c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the pipeline on the full train set costs too much ram, my laptop can’t do it\n",
    "# So we just train a final model, still with the subsampled dataset and the optimal parameters\n",
    "# And we generate the final predictions with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fdd55a7d0d09cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros(tx_train.shape[1])\n",
    "final_w, final_loss = reg_logistic_regression(y_tr, tx_train, best_hps['lambda_'], initial_w, 100, best_hps['gamma'])\n",
    "\n",
    "tx_test = build_tx(x_te)[:, best_features_to_keep]\n",
    "# Sigmoid gives a value between 0 and 1\n",
    "test_probs = sigmoid(tx_test @ final_w)\n",
    "# We round to the nearest to get our prediction in {0,1}\n",
    "test_preds = np.round(test_probs)\n",
    "# Transform them into {-1,1} for submission\n",
    "test_preds[test_preds == 0] = -1\n",
    "create_csv_submission(te_id, test_preds, 'full_test_preds_subsampled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8112996f-7550-427d-8c23-3ab59018f002",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
